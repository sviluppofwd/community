{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1Vmew4FaL23pWEhgoEiDAmj63oxNspeY4",
      "authorship_tag": "ABX9TyPxC2/LbJ5eIPn1dIRFCLrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sviluppofwd/community/blob/master/prova5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"YOUR_CSE_ID\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"YOUR_QUERY\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M9WSvUMJ_t1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install google-api-python-client --use-deprecated=legacy-resolver\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"  # Sostituisci con la tua chiave API\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(\n",
        "    q=\"UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO FRESH CHEESE ITALIAN BLEND, UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO Cheese Whole Foods Market, Inc. 365 EVERYDAY VALUE United States \",\n",
        "    cx=\"f6f517e4450dc4281\").execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTnwpIb8fv4",
        "outputId": "d67256c8-84a0-42b7-f78f-714be3e8079a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\" in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2024.8.30)\n",
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "{'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO FRESH CHEESE ITALIAN BLEND, UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO Cheese Whole Foods Market, Inc. 365 EVERYDAY VALUE United States ', 'totalResults': '20', 'searchTerms': 'UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO FRESH CHEESE ITALIAN BLEND, UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO Cheese Whole Foods Market, Inc. 365 EVERYDAY VALUE United States ', 'count': 10, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}], 'nextPage': [{'title': 'Google Custom Search - UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO FRESH CHEESE ITALIAN BLEND, UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO Cheese Whole Foods Market, Inc. 365 EVERYDAY VALUE United States ', 'totalResults': '20', 'searchTerms': 'UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO FRESH CHEESE ITALIAN BLEND, UNSMOKED PROVOLONE CHEESE, PARMESAN CHEESE & ASIAGO Cheese Whole Foods Market, Inc. 365 EVERYDAY VALUE United States ', 'count': 10, 'startIndex': 11, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}]}, 'context': {'title': 'FoodSearch'}, 'searchInformation': {'searchTime': 0.244618, 'formattedSearchTime': '0.24', 'totalResults': '20', 'formattedTotalResults': '20'}, 'items': [{'kind': 'customsearch#result', 'title': '365 by Whole Foods Market, Blend 3 Cheese Shred ... - Amazon.com', 'htmlTitle': '365 by Whole Foods Market, Blend 3 Cheese Shred ... - Amazon.com', 'link': 'https://www.amazon.com/365-Everyday-Value-Cheese-Blend/dp/B074H64BW7', 'displayLink': 'www.amazon.com', 'snippet': 'Our standards are what set us apart, and our quality is what keeps us stocking pantries, fridges and freezers with the best natural and organic 365 products\\xa0...', 'htmlSnippet': 'Our standards are what set <b>us</b> apart, and our quality is what keeps <b>us</b> stocking pantries, fridges and freezers with the best natural and organic <b>365</b> products&nbsp;...', 'formattedUrl': 'https://www.amazon.com/365-Everyday-Value-Cheese-Blend/.../B074H64...', 'htmlFormattedUrl': 'https://www.amazon.com/<b>365</b>-<b>Everyday</b>-<b>Value</b>-<b>Cheese</b>-<b>Blend</b>/.../B074H64...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTk2qP2BlmhH22oMqp46jpUyQX3t22If61Df2wLrKIksboOdnCGK9hu5eL9&s', 'width': '224', 'height': '225'}], 'metatags': [{'theme-color': '#131921', 'viewport': 'width=device-width, maximum-scale=2, minimum-scale=1, initial-scale=1, shrink-to-fit=no', 'title': 'Amazon.com: 365 by Whole Foods Market, Blend 3 Cheese Shred, 5 Ounce : Grocery & Gourmet Food', 'encrypted-slate-token': 'AnYxCOylCTX1QzYHKVThU29I3aKbpNGUgS9h0Ucerb1PTBJ+wDdFuBGrqp6dVLUqVjGDRpeHRLQz/pXAiDVpPzd0kDlqRmAW/S+Ta/yc4GYYwjRRMHyEz4t8fIe6SMZXCg6jcU369DA97/qmPSx5F0mZU2Z7DQN2tPEyae7KDqLZmjfYmgq+gM1IOm5XmBr5U9wr4aioxMIKYD0fFc0U2hcgKyCceyvX0EOaukTRU8ptlQvEuVWgdxWPN5YI8D6hxnNsn30uIjr8emZhXhAZUGXBbGE='}], 'cse_image': [{'src': 'https://m.media-amazon.com/images/I/71FjNf+KVtL._AC_UF894,1000_QL80_.jpg'}]}}, {'kind': 'customsearch#result', 'title': 'List of Protected Designation of Origin products by country - Wikipedia', 'htmlTitle': 'List of Protected Designation of Origin products by country - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/List_of_Protected_Designation_of_Origin_products_by_country', 'displayLink': 'en.wikipedia.org', 'snippet': \"An extensive list of registered PDO's is available in eAmbrosia, the official register of the European Commission. More information is published in GIview, a\\xa0...\", 'htmlSnippet': 'An extensive list of registered PDO&#39;s is available in eAmbrosia, the official register <b>of the</b> European Commission. More information is published in GIview, a&nbsp;...', 'formattedUrl': 'https://en.wikipedia.org/.../List_of_Protected_Designation_of_Origin_prod...', 'htmlFormattedUrl': 'https://en.wikipedia.org/.../List_of_Protected_Designation_of_Origin_prod...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRY_bWRoVywfAgEE8PLx6umaze0FMQDqPRVx6Gm6m2DGEpBLEhsMwMfKg&s=0', 'width': '107', 'height': '107'}], 'metatags': [{'referrer': 'origin', 'theme-color': '#eaecf0', 'og:type': 'website', 'viewport': 'width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=0.25, maximum-scale=5.0', 'og:title': 'List of Protected Designation of Origin products by country - Wikipedia', 'format-detection': 'telephone=no'}], 'cse_image': [{'src': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/%D7%9E%D7%A1%D7%98%D7%99%D7%A7%D7%90_-_2.jpg/220px-%D7%9E%D7%A1%D7%98%D7%99%D7%A7%D7%90_-_2.jpg'}]}}, {'kind': 'customsearch#result', 'title': 'CHEESE', 'htmlTitle': '<b>CHEESE</b>', 'link': 'https://www.robinhoodprovisions.com/pdf/RobinhoodGourmetCheeseList.pdf', 'displayLink': 'www.robinhoodprovisions.com', 'snippet': \"5-8 MONTH CHEDDAR RUBBED W/ HONEY & SEA SALT. WASHINGTON. BEECHER'S. HANDMADE CHEESE IN SEATTLE MAKES FRESH ARTISANAL CHEESES DAILY IN THEIR RETAIL STORE. FREE\\xa0...\", 'htmlSnippet': '5-8 MONTH <b>CHEDDAR</b> RUBBED W/ HONEY &amp; SEA SALT. WASHINGTON. BEECHER&#39;S. HANDMADE <b>CHEESE</b> IN SEATTLE MAKES <b>FRESH</b> ARTISANAL <b>CHEESES DAILY</b> IN THEIR RETAIL <b>STORE</b>. FREE&nbsp;...', 'formattedUrl': 'https://www.robinhoodprovisions.com/.../RobinhoodGourmetCheeseList.pd...', 'htmlFormattedUrl': 'https://www.robinhoodprovisions.com/.../RobinhoodGourmet<b>Cheese</b>List.pd...', 'pagemap': {'metatags': [{'moddate': 'D:20120109175158Z', 'creationdate': 'D:20120109175138Z', 'creator': 'Nitro PDF Reader  (1. 3. 2. 1)', 'producer': 'Nitro PDF Reader  (1. 3. 2. 1)'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Generations of Cooking', 'htmlTitle': 'Generations of Cooking', 'link': 'https://bychoice.com/GOC_cookbook_January_2010.pdf', 'displayLink': 'bychoice.com', 'snippet': 'Jan 17, 2010 ... ... Fresh Salmon 157. Guacamole Avocado Dip 159. Hog Island Oyster Recipes 160. Humboldt Fog and Cambozola Cheese and Italian Crostini Crackers 163.', 'htmlSnippet': 'Jan 17, 2010 <b>...</b> ... <b>Fresh</b> Salmon 157. Guacamole Avocado Dip 159. Hog Island Oyster Recipes 160. Humboldt Fog and Cambozola <b>Cheese</b> and <b>Italian</b> Crostini Crackers 163.', 'formattedUrl': 'https://bychoice.com/GOC_cookbook_January_2010.pdf', 'htmlFormattedUrl': 'https://bychoice.com/GOC_cookbook_January_2010.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSl1kqDGteclmmfJ9K2DOzHoNP47Vgkz0WZb2aWRWGSwnHQR6bFmkfbNaz6&s', 'width': '265', 'height': '191'}], 'metatags': [{'moddate': \"D:20100117114249-08'00'\", 'creationdate': \"D:20100117113958-08'00'\", 'creator': 'PScript5.dll Version 5.2', 'author': 'Administrator', 'producer': 'Acrobat Distiller 7.0.5 (Windows)', 'title': 'Title_Generations_of_cooking.fm'}], 'cse_image': [{'src': 'x-raw-image:///13fe5c4baa8893cabba65c83141dbe61e34dc012af3ba35de41030e352050052'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'A Risk Profile of Dairy Products in Australia', 'htmlTitle': 'A Risk Profile of Dairy Products in Australia', 'link': 'https://www.foodstandards.govt.nz/sites/default/files/food-standards-code/proposals/Documents/P296-DairyPPPS-FAR-Attach2.pdf', 'displayLink': 'www.foodstandards.govt.nz', 'snippet': 'Aug 9, 2006 ... However, the effectiveness of pasteurisation is dependent upon the microbiological status of the incoming raw milk. Control of risk factors on-\\xa0...', 'htmlSnippet': 'Aug 9, 2006 <b>...</b> However, the effectiveness of pasteurisation is dependent upon the microbiological status <b>of the</b> incoming raw milk. Control of risk factors on-&nbsp;...', 'formattedUrl': 'https://www.foodstandards.govt.nz/.../P296-DairyPPPS-FAR-Attach2.pdf', 'htmlFormattedUrl': 'https://www.<b>foods</b>tandards.govt.nz/.../P296-DairyPPPS-FAR-Attach2.pdf', 'pagemap': {'metatags': [{'moddate': \"D:20130919091307+10'00'\", 'creator': 'Microsoft® Word 2010', 'creationdate': \"D:20130919091307+10'00'\", 'producer': 'Microsoft® Word 2010'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Microbiology and Technology of Fermented Foods', 'htmlTitle': 'Microbiology and Technology of Fermented <b>Foods</b>', 'link': 'https://fenix.isa.ulisboa.pt/downloadFile/563022967866251/Microbiology%20and%20Fermentation%20of%20Fermented%20Foods.pdf', 'displayLink': 'fenix.isa.ulisboa.pt', 'snippet': 'The IFT Press series reflects the mission of the Institute of Food Technologists—advancing the science and technology of food through the exchange of\\xa0...', 'htmlSnippet': 'The IFT Press series reflects the mission <b>of the</b> Institute of <b>Food</b> Technologists—advancing the science and technology of <b>food</b> through the exchange of&nbsp;...', 'formattedUrl': 'https://fenix.isa.ulisboa.pt/.../Microbiology%20and%20Fermentation%20of...', 'htmlFormattedUrl': 'https://fenix.isa.ulisboa.pt/.../Microbiology%20and%20Fermentation%20of...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTmoQeVhCKj2sN0ZbnvGcNtS57n1Z3xoylpAN6wlsmH37CmYPewLG8tByvP&s', 'width': '185', 'height': '273'}], 'cse_image': [{'src': 'x-raw-image:///bc4a29f791519831694fba591e8a314722493680f2fd172feffe61defe2ae075'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'essentials of food science', 'htmlTitle': 'essentials of <b>food</b> science', 'link': 'https://k8449r.weebly.com/uploads/3/0/7/3/30731055/28_essentials_of_food_science-signed.pdf', 'displayLink': 'k8449r.weebly.com', 'snippet': '2008 Springer Science+Business Media, LLC. All rights reserved. This work may not be translated or copied in whole or in part without the written permission\\xa0...', 'htmlSnippet': '2008 Springer Science+Business Media, LLC. All rights reserved. This work may not be translated or copied in <b>whole</b> or in part without the written permission&nbsp;...', 'formattedUrl': 'https://k8449r.weebly.com/.../28_essentials_of_food_science-signed.pdf', 'htmlFormattedUrl': 'https://k8449r.weebly.com/.../28_essentials_of_food_science-signed.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfMjS5jsg37QpCEHqIVVr8nHPbzFR-Gb9l5N4nsGHGLqT4TpsE39hVYYg&s', 'width': '186', 'height': '270'}], 'metatags': [{'moddate': \"D:20170814232138+05'30'\", 'creationdate': \"D:20080601215441-05'00'\", 'creator': 'Adobe Acrobat 7.08', 'producer': 'Adobe Acrobat 7.08 Image Conversion Plug-in'}], 'cse_image': [{'src': 'x-raw-image:///fef677ba4ee62c3e65cb1e90b6d59405dba628176e652d9789fb966dbf7854fa'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Untitled', 'htmlTitle': 'Untitled', 'link': 'https://link.springer.com/content/pdf/10.1007/978-0-387-69940-0.pdf', 'displayLink': 'link.springer.com', 'snippet': '2008 Springer Science+Business Media, LLC. All rights reserved. This work may not be translated or copied in whole or in part without the written permission\\xa0...', 'htmlSnippet': '2008 Springer Science+Business Media, LLC. All rights reserved. This work may not be translated or copied in <b>whole</b> or in part without the written permission&nbsp;...', 'formattedUrl': 'https://link.springer.com/content/pdf/10.1007/978-0-387-69940-0.pdf', 'htmlFormattedUrl': 'https://link.springer.com/content/pdf/10.1007/978-0-387-69940-0.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTgGTTnSWrPKvi_KfAhJmMX5aYNxi6t4HGGxTZQGDgddd7tTkvHefpe9J4N&s', 'width': '188', 'height': '268'}], 'metatags': [{'moddate': \"D:20071202163309+05'30'\", 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20071201195441+05'30'\", 'producer': 'Acrobat Distiller 7.0 (Windows)'}], 'cse_image': [{'src': 'x-raw-image:///7dcf0e1f3c6569ec495da1e8c53f9e9b8f6cf92144244fa53da71d0b660b734e'}]}}, {'kind': 'customsearch#result', 'title': 'Food Microbiology', 'htmlTitle': '<b>Food</b> Microbiology', 'link': 'https://repository.poltekkes-kaltim.ac.id/1145/1/food-microbiology-3rd-ed.pdf', 'displayLink': 'repository.poltekkes-kaltim.ac.id', 'snippet': 'Since our subject is broad, covering a diversity of topics from viruses to helminths (by way of the bacteria) and from pathogenicity to physical chemistry, this\\xa0...', 'htmlSnippet': 'Since our subject is broad, covering a diversity of topics from viruses to helminths (by way of the bacteria) and from pathogenicity to physical chemistry, this&nbsp;...', 'formattedUrl': 'https://repository.poltekkes-kaltim.ac.id/.../1/food-microbiology-3rd-ed.pdf', 'htmlFormattedUrl': 'https://repository.poltekkes-kaltim.ac.id/.../1/food-microbiology-3rd-ed.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT1AMLGzK1XCQMC8aBpCoi4mZV6nxGdoZ69BxITR7Neg7mIzHcimRJdp_Hw&s', 'width': '183', 'height': '276'}], 'metatags': [{'moddate': \"D:20090126193652+01'00'\", 'creationdate': \"D:20090127014528+08'00'\", 'creator': 'UnknownApplication', 'producer': 'GPL Ghostscript 8.61', 'title': 'Untitled'}], 'cse_image': [{'src': 'x-raw-image:///1941133c6d18c97565bf55c4a541f3039f32d6468ac9047bb07c863b39584f36'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Full text of \"The Culinary Institute Of America The Professional Chef\"', 'htmlTitle': 'Full text <b>of &quot;The</b> Culinary Institute Of America The Professional Chef&quot;', 'link': 'https://archive.org/stream/theculinaryinstituteofamericatheprofessionalchef/The%20Culinary%20Institute%20of%20America%20-%20The%20Professional%20Chef_djvu.txt', 'displayLink': 'archive.org', 'snippet': \"Very popular in the United States Table cheese. Great melting cheese MANCHEGO Whole sheep's milk. White to yellowish wheel; brownish-gray basket-weave rind.\", 'htmlSnippet': 'Very popular in the <b>United States</b> Table <b>cheese</b>. Great melting <b>cheese</b> MANCHEGO <b>Whole</b> sheep&#39;s milk. <b>White</b> to yellowish wheel; brownish-gray basket-weave rind.', 'formattedUrl': 'https://archive.org/.../The%20Culinary%20Institute%20of%20America%20...', 'htmlFormattedUrl': 'https://archive.org/.../The%20Culinary%20Institute%20of%20America%20...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMEU1bvrdSBO8D-owlCeF0-EvarcU86Qw0r1NPRqy8lbFhDx64xMGOPA&s=0', 'width': '101', 'height': '101'}], 'metatags': [{'viewport': 'width=device-width, initial-scale=1.0', 'mediatype': 'texts'}], 'cse_image': [{'src': 'https://archive.org/services/img/metropolitanmuseumofart-gallery'}]}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definire la directory dei file Excel\n",
        "excel_dir = \"/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach\"\n",
        "\n",
        "# Creare un elenco di tutti i file Excel nella directory, ordinati in modo crescente\n",
        "excel_files = [file for file in os.listdir(excel_dir) if file.endswith(\".xlsx\")]\n",
        "excel_files.sort(key=lambda f: int(re.search(r'\\d+', f).group()))\n",
        "\n",
        "# Creare un dizionario per memorizzare i dati di ogni file Excel\n",
        "all_data = {}\n",
        "\n",
        "# Eseguire un ciclo su tutti i file Excel e leggere i dati in un DataFrame pandas\n",
        "for excel_file in excel_files:\n",
        "    file_path = os.path.join(excel_dir, excel_file)\n",
        "    df = pd.read_excel(file_path)\n",
        "    all_data[excel_file] = df\n",
        "\n",
        "# Creare un nuovo file Excel e aggiungere un foglio di lavoro\n",
        "writer = pd.ExcelWriter(\"/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/output3.xlsx\", engine=\"openpyxl\")\n",
        "\n",
        "# Definisci l'ordine delle colonne desiderato usando gli indici\n",
        "column_order = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "# Concatena i DataFrame e riordina le colonne usando iloc\n",
        "combined_df = pd.concat(all_data.values(), ignore_index=True)\n",
        "combined_df = combined_df.iloc[:, column_order]\n",
        "\n",
        "# Scrivi il DataFrame nel foglio di lavoro\n",
        "combined_df.to_excel(writer, sheet_name=\"Foglio1\", index=False)\n",
        "\n",
        "# Salvare il file Excel\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJb7Nvu3B1AQ",
        "outputId": "9a5a7cff-95c6-4ce8-ad3b-6b073ad930cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6RwJ9_g7B7NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install google-api-python-client --use-deprecated=legacy-resolver\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# (Start) read file excel\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path ='/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggi.xlsx';\n",
        "df = pd.read_excel(file_path, skiprows=1,nrows=20) # skiprows=(0) oppure (1) per saltare la prima riga (intestazioni)\n",
        "primi_n_righi = df.head(20)\n",
        "primi_n_righi['Link1'] = ''\n",
        "primi_n_righi['Link2'] = ''\n",
        "primi_n_righi['Link3'] = ''\n",
        "import time\n",
        "# (End) read file excel\n",
        "\n",
        "# La tua chiave API\n",
        "# my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "# MyGoogleApi = AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\n",
        "# AppLOrologino =  AIzaSyC7lS9hy3qwr2aKMaULaLCY5-itY6_YOrM\n",
        "# AppGoMeteo =  AIzaSyBE5YwbSjhrZrfw7fBZh3gXRAX_zu-LfT8\n",
        "\n",
        "# ItalianSolving =  AIzaSyA0Zwo7oZO75mkwQVhLoGp6igRRSYLLdWM\n",
        "\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "def google_search(query, **kwargs):\n",
        "  \"\"\"Cerca su Google e restituisce i risultati.\n",
        "  Args:\n",
        "    query: La stringa di query.\n",
        "    **kwargs: Argomenti di parole chiave aggiuntivi da passare al metodo list().\n",
        "      Vedi la documentazione dell'API Custom Search per un elenco completo di opzioni.\n",
        "  Returns:\n",
        "    I risultati della ricerca.\n",
        "  \"\"\"\n",
        "  # res = service.cse().list(q=query, cx=my_cse_id, **kwargs).execute()\n",
        "  res = service.cse().list(q=query, **kwargs).execute()\n",
        "  return res\n",
        "\n",
        "# (Start) Loop\n",
        "for index, row in primi_n_righi.iterrows():\n",
        "#  query = str(row['Description'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandOwner'])+\",\"+str(row['Brand'])+\",\"+str(row['MarketCountry']) # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "  query = str(row.iloc[1]) +\",\"+str(row.iloc[2]) +\",\"+str(row.iloc[3]) +\",\"+str(row.iloc[4]) +\",\"+str(row.iloc[5])  # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "\n",
        "# Esempio di utilizzo\n",
        "  time.sleep(3)\n",
        "  results = google_search(query,searchType=\"image\",  num=1, safe=\"off\", lr=\"lang_en\")\n",
        "# results = google_search(\"PARMESAN SHREDDED CHEESE, Cheese, Kraft Heinz Foods Company,United States\",sort=\"date:d:s\",searchType=\"image\", num=10, safe=\"off\", lr=\"lang_en\")\n",
        "\n",
        "# Stampa i risultati\n",
        "  for i, item in enumerate(results.get(\"items\", [])):\n",
        "    colonna_link = f\"Link{i+1}\"\n",
        "    primi_n_righi.loc[index, colonna_link] = item.get(\"link\")\n",
        "    # print(item.get(\"title\"))\n",
        "    print(item.get(\"link\"))\n",
        "    # print(item.get(\"snippet\"))\n",
        "    # Stampa la data (se disponibile)\n",
        "    \"\"\"\n",
        "    if \"metadata\" in item and \"date\" in item[\"metadata\"]:\n",
        "      print(\"Data:\", item[\"metadata\"][\"date\"])\n",
        "    else:\n",
        "      print(\"Data non disponibile\")\n",
        "    \"\"\"\n",
        "    # print(\"-\" * 20)\n",
        "# (End Loop)\n",
        "primi_n_righi.to_excel('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggiImgPublicTotale.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TCLqVeCBIoS0",
        "outputId": "c65c0aef-d1f0-4591-a9ff-d28f5fb53919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\" in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2024.8.30)\n",
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b1ecab3d445a>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link1'] = ''\n",
            "<ipython-input-2-b1ecab3d445a>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link2'] = ''\n",
            "<ipython-input-2-b1ecab3d445a>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link3'] = ''\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "<HttpError 400 when requesting https://customsearch.googleapis.com/customsearch/v1?q=PARMESAN+CHEESE%2CCheese%2CWhole+Foods+Market%2C+Inc.%2C365%2CUnited+States&searchType=image&num=1&safe=off&lr=lang_en&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Request contains an invalid argument.\">",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b1ecab3d445a>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Esempio di utilizzo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearchType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lang_en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# results = google_search(\"PARMESAN SHREDDED CHEESE, Cheese, Kraft Heinz Foods Company,United States\",sort=\"date:d:s\",searchType=\"image\", num=10, safe=\"off\", lr=\"lang_en\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b1ecab3d445a>\u001b[0m in \u001b[0;36mgoogle_search\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m# res = service.cse().list(q=query, cx=my_cse_id, **kwargs).execute()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://customsearch.googleapis.com/customsearch/v1?q=PARMESAN+CHEESE%2CCheese%2CWhole+Foods+Market%2C+Inc.%2C365%2CUnited+States&searchType=image&num=1&safe=off&lr=lang_en&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Request contains an invalid argument.\">"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install google-api-python-client --use-deprecated=legacy-resolver\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# (Start) read file excel\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path ='/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggi.xlsx';\n",
        "df = pd.read_excel(file_path, skiprows=1,nrows=20) # skiprows=(0) oppure (1) per saltare la prima riga (intestazioni)\n",
        "primi_n_righi = df.head(20)\n",
        "primi_n_righi['Link1'] = ''\n",
        "primi_n_righi['Link2'] = ''\n",
        "primi_n_righi['Link3'] = ''\n",
        "import time\n",
        "# (End) read file excel\n",
        "\n",
        "# La tua chiave API\n",
        "# my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "# MyGoogleApi = AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\n",
        "# AppLOrologino =  AIzaSyC7lS9hy3qwr2aKMaULaLCY5-itY6_YOrM\n",
        "# AppGoMeteo =  AIzaSyBE5YwbSjhrZrfw7fBZh3gXRAX_zu-LfT8\n",
        "\n",
        "# ItalianSolving =  AIzaSyA0Zwo7oZO75mkwQVhLoGp6igRRSYLLdWM\n",
        "\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "def google_search(query, **kwargs):\n",
        "  \"\"\"Cerca su Google e restituisce i risultati.\n",
        "  Args:\n",
        "    query: La stringa di query.\n",
        "    **kwargs: Argomenti di parole chiave aggiuntivi da passare al metodo list().\n",
        "      Vedi la documentazione dell'API Custom Search per un elenco completo di opzioni.\n",
        "  Returns:\n",
        "    I risultati della ricerca.\n",
        "  \"\"\"\n",
        "  # res = service.cse().list(q=query, cx=my_cse_id, **kwargs).execute()\n",
        "  res = service.cse().list(q=query, **kwargs).execute()\n",
        "  return res\n",
        "\n",
        "# (Start) Loop\n",
        "for index, row in primi_n_righi.iterrows():\n",
        "#  query = str(row['Description'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandOwner'])+\",\"+str(row['Brand'])+\",\"+str(row['MarketCountry']) # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "  query = str(row.iloc[1]) +\",\"+str(row.iloc[2]) +\",\"+str(row.iloc[3]) +\",\"+str(row.iloc[4]) +\",\"+str(row.iloc[5])  # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "\n",
        "# Esempio di utilizzo\n",
        "  time.sleep(3)\n",
        "  results = google_search(query,searchType=\"image\",  num=1, safe=\"off\", lr=\"lang_en\")\n",
        "# results = google_search(\"PARMESAN SHREDDED CHEESE, Cheese, Kraft Heinz Foods Company,United States\",sort=\"date:d:s\",searchType=\"image\", num=10, safe=\"off\", lr=\"lang_en\")\n",
        "\n",
        "# Stampa i risultati\n",
        "  for i, item in enumerate(results.get(\"items\", [])):\n",
        "    colonna_link = f\"Link{i+1}\"\n",
        "    primi_n_righi.loc[index, colonna_link] = item.get(\"link\")\n",
        "    # print(item.get(\"title\"))\n",
        "    print(item.get(\"link\"))\n",
        "    # print(item.get(\"snippet\"))\n",
        "    # Stampa la data (se disponibile)\n",
        "    \"\"\"\n",
        "    if \"metadata\" in item and \"date\" in item[\"metadata\"]:\n",
        "      print(\"Data:\", item[\"metadata\"][\"date\"])\n",
        "    else:\n",
        "      print(\"Data non disponibile\")\n",
        "    \"\"\"\n",
        "    # print(\"-\" * 20)\n",
        "# (End Loop)\n",
        "primi_n_righi.to_excel('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggiImgPublicTotale.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c65c0aef-d1f0-4591-a9ff-d28f5fb53919",
        "id": "EpyHl1j50QEz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\" in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2024.8.30)\n",
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b1ecab3d445a>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link1'] = ''\n",
            "<ipython-input-2-b1ecab3d445a>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link2'] = ''\n",
            "<ipython-input-2-b1ecab3d445a>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link3'] = ''\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "<HttpError 400 when requesting https://customsearch.googleapis.com/customsearch/v1?q=PARMESAN+CHEESE%2CCheese%2CWhole+Foods+Market%2C+Inc.%2C365%2CUnited+States&searchType=image&num=1&safe=off&lr=lang_en&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Request contains an invalid argument.\">",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b1ecab3d445a>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Esempio di utilizzo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearchType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lang_en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# results = google_search(\"PARMESAN SHREDDED CHEESE, Cheese, Kraft Heinz Foods Company,United States\",sort=\"date:d:s\",searchType=\"image\", num=10, safe=\"off\", lr=\"lang_en\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b1ecab3d445a>\u001b[0m in \u001b[0;36mgoogle_search\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m# res = service.cse().list(q=query, cx=my_cse_id, **kwargs).execute()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://customsearch.googleapis.com/customsearch/v1?q=PARMESAN+CHEESE%2CCheese%2CWhole+Foods+Market%2C+Inc.%2C365%2CUnited+States&searchType=image&num=1&safe=off&lr=lang_en&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Request contains an invalid argument.\">"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# La tua chiave API\n",
        "# api_key = \"LA_TUA_API_KEY\"\n",
        "api_key = \"AIzaSyA0Zwo7oZO75mkwQVhLoGp6igRRSYLLdWM\"\n",
        "\n",
        "# La query di ricerca\n",
        "query = \"HOMESTYLE THREE CHEESE PARMESAN\"\n",
        "\n",
        "# L'URL dell'API Custom Search JSON\n",
        "url = f\"https://www.googleapis.com/customsearch/v1?key={api_key}&q={query}\"\n",
        "\n",
        "# Esegui la richiesta\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parsa la risposta JSON\n",
        "results = json.loads(response.text)\n",
        "print(response.text)\n",
        "# Processa i risultati\n",
        "# ..."
      ],
      "metadata": {
        "id": "K-2iWt0I0-70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install google-api-python-client --use-deprecated=legacy-resolver\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# (Start) read file excel\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path ='/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggi.xlsx';\n",
        "df = pd.read_excel(file_path, skiprows=1,nrows=100) # skiprows=(0) oppure (1) per saltare la prima riga (intestazioni)\n",
        "primi_n_righi = df.head(100)\n",
        "primi_n_righi['Link1'] = ''\n",
        "primi_n_righi['Link2'] = ''\n",
        "primi_n_righi['Link3'] = ''\n",
        "import time\n",
        "# (End) read file excel\n",
        "\n",
        "# La tua chiave API\n",
        "# my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "# MyGoogleApi = AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\n",
        "# AppLOrologino =  AIzaSyC7lS9hy3qwr2aKMaULaLCY5-itY6_YOrM\n",
        "# AppGoMeteo =  AIzaSyBE5YwbSjhrZrfw7fBZh3gXRAX_zu-LfT8\n",
        "\n",
        "# ItalianSolving =  AIzaSyA0Zwo7oZO75mkwQVhLoGp6igRRSYLLdWM\n",
        "\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "def google_search(query, **kwargs):\n",
        "  \"\"\"Cerca su Google e restituisce i risultati.\n",
        "  Args:\n",
        "    query: La stringa di query.\n",
        "    **kwargs: Argomenti di parole chiave aggiuntivi da passare al metodo list().\n",
        "      Vedi la documentazione dell'API Custom Search per un elenco completo di opzioni.\n",
        "  Returns:\n",
        "    I risultati della ricerca.\n",
        "  \"\"\"\n",
        "  res = service.cse().list(q=query, cx=my_cse_id, **kwargs).execute()\n",
        "  return res\n",
        "\n",
        "# (Start) Loop\n",
        "for index, row in primi_n_righi.iterrows():\n",
        "#  query = str(row['Description'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandedFoodCategory'])+\",\"+str(row['BrandOwner'])+\",\"+str(row['Brand'])+\",\"+str(row['MarketCountry']) # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "  query = str(row.iloc[0])+\",\" +str(row.iloc[1]) +\",\"+str(row.iloc[2]) +\",\"+str(row.iloc[3]) +\",\"+str(row.iloc[4]) +\",\"+str(row.iloc[5])  # Sostituisci 'ColonnaQuery' con il nome della colonna contenente le query\n",
        "\n",
        "# Esempio di utilizzo\n",
        "  time.sleep(3)\n",
        "  results = google_search(query,searchType=\"image\",  num=1, safe=\"off\", lr=\"lang_en\")\n",
        "# results = google_search(\"PARMESAN SHREDDED CHEESE, Cheese, Kraft Heinz Foods Company,United States\",sort=\"date:d:s\",searchType=\"image\", num=10, safe=\"off\", lr=\"lang_en\")\n",
        "\n",
        "# Stampa i risultati\n",
        "  for i, item in enumerate(results.get(\"items\", [])):\n",
        "    colonna_link = f\"Link{i+1}\"\n",
        "    primi_n_righi.loc[index, colonna_link] = item.get(\"link\")\n",
        "    # print(item.get(\"title\"))\n",
        "    print(item.get(\"link\"))\n",
        "    # print(item.get(\"snippet\"))\n",
        "    # Stampa la data (se disponibile)\n",
        "    \"\"\"\n",
        "    if \"metadata\" in item and \"date\" in item[\"metadata\"]:\n",
        "      print(\"Data:\", item[\"metadata\"][\"date\"])\n",
        "    else:\n",
        "      print(\"Data non disponibile\")\n",
        "    \"\"\"\n",
        "    # print(\"-\" * 20)\n",
        "# (End Loop)\n",
        "primi_n_righi.to_excel('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/GoogleApiSeach/nuovaformaggiCodeImg.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79963a5e-ef50-4232-ccef-3392aa0d9f62",
        "id": "MnkyT5BF0Q3r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\" in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2024.8.30)\n",
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-123bab618f33>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link1'] = ''\n",
            "<ipython-input-12-123bab618f33>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link2'] = ''\n",
            "<ipython-input-12-123bab618f33>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  primi_n_righi['Link3'] = ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://i.ebayimg.com/images/g/FWwAAOSwtYVmStRh/s-l400.jpg\n",
            "http://site.unbeatablesale.com/KEHE42602.JPG\n",
            "http://site.unbeatablesale.com/KEHE42602.JPG\n",
            "https://www.italfoodsinc.com/uploads/brands/cache/_c829a70513_240x145.png\n",
            "https://www.italfoodsinc.com/uploads/products/cache/205084_240x315.jpg\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "https://www.italfoodsinc.com/uploads/products/cache/205084_240x315.jpg\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "https://www.italfoodsinc.com/uploads/products/cache/205092_240x315.jpg\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "https://www.italfoodsinc.com/uploads/brands/cache/_c829a70513_240x145.png\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n",
            "x-raw-image:///56a4f39cd75278bbf94f80a79e6acc2931e495bf032a0b0d77693d00013cfece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"pizza \", cx=\"000495869231624199701:9elco2zxzks\",searchType=\"image\",num=5).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L8kJEk0TbrZc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "848da78d-3a8d-45bc-8a17-c229065d7f18",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "<HttpError 404 when requesting https://customsearch.googleapis.com/customsearch/v1?q=pizza+&cx=000495869231624199701%3A9elco2zxzks&searchType=image&num=5&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Requested entity was not found.\">",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-11ce6f2f4cd3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Effettua una query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pizza \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"000495869231624199701:9elco2zxzks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearchType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Stampa i risultati\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 404 when requesting https://customsearch.googleapis.com/customsearch/v1?q=pizza+&cx=000495869231624199701%3A9elco2zxzks&searchType=image&num=5&key=AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU&alt=json returned \"Requested entity was not found.\">"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"YOUR_API_KEY\"  # Sostituisci con la tua chiave API\n",
        "\n",
        "# ID del tuo nuovo motore di ricerca personalizzato pubblico\n",
        "my_cse_id = \"YOUR_NEW_CSE_ID\"  # Sostituisci con l'ID del tuo nuovo motore di ricerca\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(\n",
        "    q=\"pizza \",\n",
        "    cx=my_cse_id,  # Utilizza l'ID del tuo nuovo motore di ricerca\n",
        "    searchType=\"image\",\n",
        "    num=5\n",
        ").execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9XjLMWjr_h_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"YOUR_API_KEY\"  # Sostituisci con la tua chiave API\n",
        "\n",
        "# ID del tuo nuovo motore di ricerca personalizzato pubblico\n",
        "my_cse_id = \"YOUR_NEW_CSE_ID\"  # Sostituisci con l'ID del tuo nuovo motore di ricerca\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(\n",
        "    q=\"pizza \",\n",
        "    cx=my_cse_id,  # Utilizza l'ID del tuo nuovo motore di ricerca\n",
        "    searchType=\"image\",\n",
        "    num=5\n",
        ").execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i-Ay62d_AWRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"YOUR_API_KEY\"  # Sostituisci con la tua chiave API\n",
        "\n",
        "# ID del tuo nuovo motore di ricerca personalizzato pubblico\n",
        "my_cse_id = \"YOUR_NEW_CSE_ID\"  # Sostituisci con l'ID del tuo nuovo motore di ricerca\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(\n",
        "    q=\"pizza \",\n",
        "    cx=my_cse_id,  # Utilizza l'ID del tuo nuovo motore di ricerca\n",
        "    searchType=\"image\",\n",
        "    num=5\n",
        ").execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tfDH6DHfAczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uo41h6eQzw7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"PARMESAN & BASIL SOFT SPREADABLE CHEESE, PARMESAN & BASIL, CHEESE, Savencia Cheese USA LLC,ALOUETTE,United States\", cx=my_cse_id,\n",
        "    num=5).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a312ab-b792-4c64-dc6d-e1a7b0be0f26",
        "collapsed": true,
        "id": "Fza9gyF8pQez"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "{'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - PARMESAN & BASIL SOFT SPREADABLE CHEESE, PARMESAN & BASIL, CHEESE, Savencia Cheese USA LLC,ALOUETTE,United States', 'totalResults': '5', 'searchTerms': 'PARMESAN & BASIL SOFT SPREADABLE CHEESE, PARMESAN & BASIL, CHEESE, Savencia Cheese USA LLC,ALOUETTE,United States', 'count': 5, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}]}, 'context': {'title': 'FoodSearch'}, 'searchInformation': {'searchTime': 0.13697, 'formattedSearchTime': '0.14', 'totalResults': '5', 'formattedTotalResults': '5'}, 'items': [{'kind': 'customsearch#result', 'title': 'Untitled', 'htmlTitle': 'Untitled', 'link': 'https://www.cheesereporter.com/CR/US%20Cheese%20Contest%20Booklet.pdf', 'displayLink': 'www.cheesereporter.com', 'snippet': 'Basil Soft Spreadable Cheese ... Alouette Cheese, Savencia Cheese USA, New. Holland, PA, Alouette Spread\\xa0...', 'htmlSnippet': '<b>Basil Soft Spreadable Cheese</b> ... <b>Alouette Cheese</b>, <b>Savencia Cheese USA</b>, New. Holland, PA, <b>Alouette</b> Spread&nbsp;...', 'formattedUrl': 'https://www.cheesereporter.com/.../US%20Cheese%20Contest%20Booklet.p...', 'htmlFormattedUrl': 'https://www.<b>cheese</b>reporter.com/.../US%20<b>Cheese</b>%20Contest%20Booklet.p...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRXEQ6Gz8ztLk7UV4IPiXyblSJew-fs4_fhE5JRESbRwnHu2iYHKIeJf42s&s', 'width': '286', 'height': '176'}], 'metatags': [{'moddate': \"D:20190405120620-05'00'\", 'creationdate': \"D:20190402160939-05'00'\", 'creator': 'Adobe InDesign CC 13.1 (Macintosh)', 'producer': 'Adobe PDF Library 15.0'}], 'cse_image': [{'src': 'x-raw-image:///96dd0fa519f95ad1fd19b6ada9f1b03a5ea19dae1fdb10eeb58dc9794e1ca5c2'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': '2023 American Cheese Society Judging & Competition Award ...', 'htmlTitle': '2023 American <b>Cheese</b> Society Judging &amp; Competition Award ...', 'link': 'https://www.cheesesociety.org/wp-content/uploads/2023/07/ACS-JC-Winner-List-2023.pdf', 'displayLink': 'www.cheesesociety.org', 'snippet': 'Savencia Cheese USA. Kolb-Lena Team. New Holland. PA. 2nd Place. BC: Soft Ripened ... Pine Tree Parmesan Balfour Farm LLC. Heather Donahue and. Olivia Field.', 'htmlSnippet': '<b>Savencia Cheese USA</b>. Kolb-Lena Team. New Holland. PA. 2nd Place. BC: <b>Soft</b> Ripened ... Pine Tree <b>Parmesan</b> Balfour Farm <b>LLC</b>. Heather Donahue and. Olivia Field.', 'formattedUrl': 'https://www.cheesesociety.org/wp-content/.../ACS-JC-Winner-List-2023.pd...', 'htmlFormattedUrl': 'https://www.<b>cheese</b>society.org/wp-content/.../ACS-JC-Winner-List-2023.pd...', 'pagemap': {'metatags': [{'moddate': \"D:20230717200550+00'00'\", 'creator': 'Microsoft Word', 'creationdate': \"D:20230717200550+00'00'\", 'author': 'Kori Sulewski'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': '2022 World Championship - Cheese Contest', 'htmlTitle': '2022 World Championship - <b>Cheese</b> Contest', 'link': 'https://www.cheesereporter.com/CR/World%20Cheese%20Contest%20Booklet%20-%202022.pdf', 'displayLink': 'www.cheesereporter.com', 'snippet': 'Soft-Ripened Cheeses. BEST OF CLASS. Jasper Hill Farm, Greensboro Bend, VT, United. States, Harbison | 99.23. SECOND AWARD. Savencia Cheese USA, Lena, IL,\\xa0...', 'htmlSnippet': '<b>Soft</b>-Ripened <b>Cheeses</b>. BEST OF CLASS. Jasper Hill Farm, Greensboro Bend, VT, <b>United</b>. <b>States</b>, Harbison | 99.23. SECOND AWARD. <b>Savencia Cheese USA</b>, Lena, IL,&nbsp;...', 'formattedUrl': 'https://www.cheesereporter.com/.../World%20Cheese%20Contest%20Bookl...', 'htmlFormattedUrl': 'https://www.<b>cheese</b>reporter.com/.../World%20<b>Cheese</b>%20Contest%20Bookl...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTpQDrdA7xeb0sDMd_Gb2jglCaOVRLa_4-hD5tRozWRDCnwP-d5S2tGPtw&s', 'width': '237', 'height': '213'}], 'metatags': [{'moddate': \"D:20220330095831-05'00'\", 'creationdate': \"D:20220330095718-05'00'\", 'creator': 'Adobe InDesign 17.1 (Windows)', 'gts_pdfxconformance': 'PDF/X-1a:2001', 'producer': 'Adobe PDF Library 16.0.5', 'title': 'contest-booklet-CHREP-2022.indb', 'gts_pdfxversion': 'PDF/X-1:2001'}], 'cse_image': [{'src': 'x-raw-image:///8e957381f4cf0d7d97644e6fc7175ff62a93912130463a08b23a6839d6109fdb'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Stephen Kentner - Logistics Manager - HOPE Foods | LinkedIn', 'htmlTitle': 'Stephen Kentner - Logistics Manager - HOPE Foods | LinkedIn', 'link': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'displayLink': 'www.linkedin.com', 'snippet': 'Alouette Cheese USA LLC. Feb 2015 - 2017 2 years. Lena, Illinois, United States. Lead the production sanitation team in assisting production team, develop\\xa0...', 'htmlSnippet': '<b>Alouette Cheese USA LLC</b>. Feb 2015 - 2017 2 years. Lena, Illinois, <b>United States</b>. Lead the production sanitation team in assisting production team, develop&nbsp;...', 'formattedUrl': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'htmlFormattedUrl': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQQ8zmtumj8p2VcmHSab2SbFVwlRqWluotc6R6C1WSIhZd_nDajX0iE10zr&s', 'width': '226', 'height': '223'}], 'metatags': [{'og:image': 'https://media.licdn.com/dms/image/v2/D5603AQHr6L_kIxSomg/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1681867065667?e=2147483647&v=beta&t=KCTNZPEdxhYNVkHyJpDhcqZtuuDvAUSjOWaLD-Eb3C8', 'twitter:card': 'summary', 'linkedin:pagetag': 'openToProvider', 'platform-worker': 'https://static.licdn.com/aero-v1/sc/h/7nirg34a8ey4y2l4rw7xgwxx4', 'al:android:package': 'com.linkedin.android', 'bingbot': 'nocache', 'locale': 'en_US', 'al:ios:url': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'og:description': 'Logistics Manager · A calculated individual that prides himself on efficiency and quality. · Experience: HOPE Foods · Education: Rasmussen College - Rockford, IL · Location: Louisville · 110 connections on LinkedIn. View Stephen Kentner’s profile on LinkedIn, a professional community of 1 billion members.', 'al:ios:app_store_id': '288429040', 'platform': 'https://static.licdn.com/aero-v1/sc/h/em0l2r6g9a33es0tv82idd4cw', 'twitter:image': 'https://media.licdn.com/dms/image/v2/D5603AQHr6L_kIxSomg/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1681867065667?e=2147483647&v=beta&t=KCTNZPEdxhYNVkHyJpDhcqZtuuDvAUSjOWaLD-Eb3C8', 'profile:last_name': 'Kentner', 'twitter:site': '@Linkedin', 'litmsprofilename': 'public-profile-frontend', 'profile:first_name': 'Stephen', 'og:type': 'profile', 'twitter:title': 'Stephen Kentner - Logistics Manager - HOPE Foods | LinkedIn', 'al:ios:app_name': 'LinkedIn', 'og:title': 'Stephen Kentner - Logistics Manager - HOPE Foods | LinkedIn', 'pagekey': 'public_profile_v3_mobile', 'al:android:url': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'viewport': 'width=device-width, initial-scale=1.0', 'twitter:description': 'Logistics Manager · A calculated individual that prides himself on efficiency and quality. · Experience: HOPE Foods · Education: Rasmussen College - Rockford, IL · Location: Louisville · 110 connections on LinkedIn. View Stephen Kentner’s profile on LinkedIn, a professional community of 1 billion members.', 'ubba': 'https://static.licdn.com/aero-v1/sc/h/9p9fe5unol6ytr6so5vasjs6m', 'og:url': 'https://www.linkedin.com/in/stephen-kentner-09589a45', 'al:android:app_name': 'LinkedIn'}], 'cse_image': [{'src': 'https://media.licdn.com/dms/image/v2/D4E22AQGHSs_a1s6V1A/feedshare-shrink_800/feedshare-shrink_800/0/1681280518066?e=2147483647&v=beta&t=Ukb_yWB5WX7gGoozEKUWGleUmzDlqmUpPM4gXaWjKnw'}], 'Person': [{}]}}, {'kind': 'customsearch#result', 'title': 'Cheese (Fresh, Natural and Processed)', 'htmlTitle': '<b>Cheese</b> (Fresh, Natural and Processed)', 'link': 'https://www.berryondairy.com/cheese.html', 'displayLink': 'www.berryondairy.com', 'snippet': 'It is a blend of Monterey Jack, Sharp Cheddar and Parmesan cheeses. The Triple Cheddar shredded blend is a mix of White Sharp Cheddar, Sharp Cheddar and Mild\\xa0...', 'htmlSnippet': 'It is a blend of Monterey Jack, Sharp <b>Cheddar</b> and <b>Parmesan cheeses</b>. The Triple <b>Cheddar</b> shredded blend is a mix of White Sharp <b>Cheddar</b>, Sharp <b>Cheddar</b> and Mild&nbsp;...', 'formattedUrl': 'https://www.berryondairy.com/cheese.html', 'htmlFormattedUrl': 'https://www.berryondairy.com/<b>cheese</b>.html', 'pagemap': {'hcard': [{'fn': 'Dairy and Food Communications, Inc.'}], 'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTl_9YOUnPQHuyZOppcK4UlwnSgxHv-huOI6tMqIed-Xptk1N1OQgJYoeU&s', 'width': '408', 'height': '123'}], 'cse_image': [{'src': 'https://www.berryondairy.com/image/50402222.png'}]}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"SPICY JALAPENO POPPER DIP, SPICY JALAPENO POPPER,CHEESE,Fish House Foods Company,United States\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bf0b6d-7514-4da5-f442-bec8e80857f5",
        "collapsed": true,
        "id": "92e9ub4ipRTZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "{'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - SPICY JALAPENO POPPER DIP, SPICY JALAPENO POPPER,CHEESE,Fish House Foods Company,United States', 'totalResults': '152000', 'searchTerms': 'SPICY JALAPENO POPPER DIP, SPICY JALAPENO POPPER,CHEESE,Fish House Foods Company,United States', 'count': 10, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}], 'nextPage': [{'title': 'Google Custom Search - SPICY JALAPENO POPPER DIP, SPICY JALAPENO POPPER,CHEESE,Fish House Foods Company,United States', 'totalResults': '152000', 'searchTerms': 'SPICY JALAPENO POPPER DIP, SPICY JALAPENO POPPER,CHEESE,Fish House Foods Company,United States', 'count': 10, 'startIndex': 11, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}]}, 'context': {'title': 'FoodSearch'}, 'searchInformation': {'searchTime': 0.669381, 'formattedSearchTime': '0.67', 'totalResults': '152000', 'formattedTotalResults': '152,000'}, 'items': [{'kind': 'customsearch#result', 'title': 'Spicy Jalapeno Popper Dip Blitzd', 'htmlTitle': '<b>Spicy Jalapeno Popper Dip</b> Blitzd', 'link': 'https://www.gfifoods.com/23062-blitzd-spicy-jalapeno-popper-dip-23062', 'displayLink': 'www.gfifoods.com', 'snippet': 'This incredible dip combines bits of spicy jalapeno, cheddar cheese, buttermilk ranch seasoning, and a cream cheese and sour cream blend.', 'htmlSnippet': 'This incredible <b>dip</b> combines bits of <b>spicy jalapeno</b>, cheddar <b>cheese</b>, buttermilk ranch seasoning, and a <b>cream cheese</b> and sour cream blend.', 'formattedUrl': 'https://www.gfifoods.com/23062-blitzd-spicy-jalapeno-popper-dip-23062', 'htmlFormattedUrl': 'https://www.gfi<b>foods</b>.com/23062-blitzd-<b>spicy</b>-<b>jalapeno</b>-<b>popper</b>-<b>dip</b>-23062', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSpZl_4IcihN4CeUulwUkf1V7hgDsocJDHC72APtWjz0wtF2xahV3al_jQ&s', 'width': '225', 'height': '225'}], 'thumbnail': [{'src': 'https://www.gfifoods.com/media/catalog/product/cache/8794a98778c2ca77788a2eb8b07fbd40/8/2/820581848293_ecommerce_zoczf4vjr3rjmd6b.jpg', 'name': 'Spicy Jalapeno Popper Dip'}], 'metatags': [{'ahrefs-site-verification': '3566900058df7e2f451314efd2a8f79d908c868f31a0974b3a722dbf6981549c', 'og:image': 'https://www.gfifoods.com/media/catalog/product/cache/c298da2f8113e2d88de7f1bc338b40fc/8/2/820581848293_ecommerce_zoczf4vjr3rjmd6b.jpg', 'og:type': 'product', 'viewport': 'width=device-width, initial-scale=1', 'og:title': 'Spicy Jalapeno Popper Dip', 'product:price:currency': 'USD', 'product:price:amount': '69.98', 'title': 'Spicy Jalapeno Popper Dip Blitzd', 'og:url': 'https://www.gfifoods.com/23062-blitzd-spicy-jalapeno-popper-dip-23062', 'og:description': 'You have found your new favorite snack with Blitzd Spicy Jalapeno Popper Dip! This incredible dip combines bits of spicy jalapeno, cheddar cheese, buttermilk ranch seasoning, and a cream cheese and sour cream blend.', 'format-detection': 'telephone=no'}], 'cse_image': [{'src': 'https://www.gfifoods.com/media/catalog/product/cache/8794a98778c2ca77788a2eb8b07fbd40/8/2/820581848293_ecommerce_zoczf4vjr3rjmd6b.jpg'}]}}, {'kind': 'customsearch#result', 'title': \"What's the best sauce to dip jalapeño poppers? - Quora\", 'htmlTitle': 'What&#39;s the best sauce to <b>dip jalapeño poppers</b>? - Quora', 'link': 'https://www.quora.com/What-s-the-best-sauce-to-dip-jalape%C3%B1o-poppers', 'displayLink': 'www.quora.com', 'snippet': \"Jan 13, 2019 ... To my trained palate, a sweet dipping sauce is the perfect juxtaposition to a crunchy, salty, and spicy jalapeño popper. Here's a short list of\\xa0...\", 'htmlSnippet': 'Jan 13, 2019 <b>...</b> To my trained palate, a sweet dipping sauce is the perfect juxtaposition to a crunchy, salty, and <b>spicy jalapeño popper</b>. Here&#39;s a short list of&nbsp;...', 'formattedUrl': 'https://www.quora.com/What-s-the-best-sauce-to-dip-jalapeño-poppers', 'htmlFormattedUrl': 'https://www.quora.com/What-s-the-best-sauce-to-<b>dip</b>-jalapeño-<b>popper</b>s', 'pagemap': {'metatags': [{'al:android:url': 'intent://www.quora.com/What-s-the-best-sauce-to-dip-jalape%C3%B1o-poppers#Intent;scheme=qhttp;package=com.quora.android;S.market_referrer=launch_url=https%3A%2F%2Fwww.quora.com%2FWhat-s-the-best-sauce-to-dip-jalape%25C3%25B1o-poppers&logging_data=uid%3DNone;end', 'theme-color': '#b92b27', 'viewport': 'initial-scale=1, maximum-scale=1, user-scalable=no, width=device-width, minimum-scale=1, viewport-fit=cover', 'al:android:package': 'com.quora.android', 'fb:pages': '255232486973', 'twitter:widgets:theme': 'light'}], 'QAPage': [{}]}}, {'kind': 'customsearch#result', 'title': 'Deep Dish Sausage & Gravy | Banquet', 'htmlTitle': 'Deep Dish Sausage &amp; Gravy | Banquet', 'link': 'https://www.banquet.com/breakfast-pies/deep-dish-sausage-gravy', 'displayLink': 'www.banquet.com', 'snippet': 'I have eaten many Banquet meals in the past and I never thought anything could top the Spicy Jalapeno Popper Bacon Mac+Cheese & Chicken Rice Bowl. But... I\\xa0...', 'htmlSnippet': 'I have eaten many Banquet meals <b>in the</b> past and I never thought anything could top the <b>Spicy Jalapeno Popper</b> Bacon Mac+<b>Cheese</b> &amp; Chicken Rice Bowl. But... I&nbsp;...', 'formattedUrl': 'https://www.banquet.com/breakfast-pies/deep-dish-sausage-gravy', 'htmlFormattedUrl': 'https://www.banquet.com/breakfast-pies/deep-dish-sausage-gravy', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRpHxYhf-fBhdG7GtMO0gR6QygtEjwMfOR7IKE0G1goIJSG30pp25uXz_ip&s', 'width': '225', 'height': '225'}], 'metatags': [{'referrer': 'strict-origin-when-cross-origin', 'og:image': 'https://www.banquet.com/sites/g/files/qyyrlu296/files/images/products/deep-dish-sausage-gravy-76431.png', 'og:type': 'product', 'og:site_name': 'Banquet', 'handheldfriendly': 'true', 'viewport': 'width=device-width, initial-scale=1.0', 'og:title': 'Deep Dish Sausage & Gravy', 'mobileoptimized': 'width', 'og:url': 'https://www.banquet.com/breakfast-pies/deep-dish-sausage-gravy', 'og:description': 'The breakfast Sausage and Gravy Pot Pie from Banquet is the perfect start to your morning. Try a Sausage & Gravy Pot Pie from Banquet for breakfast.'}], 'cse_image': [{'src': 'https://www.banquet.com/sites/g/files/qyyrlu296/files/images/products/deep-dish-sausage-gravy-76431.png'}], 'individualproduct': [{'name': 'Deep Dish Sausage & Gravy'}]}}, {'kind': 'customsearch#result', 'title': 'REVIEW: New 2024 Minnesota State Fair Foods | Minnesota Monthly', 'htmlTitle': 'REVIEW: New 2024 Minnesota <b>State</b> Fair <b>Foods</b> | Minnesota Monthly', 'link': 'https://www.minnesotamonthly.com/featured/homepage-feature/review-new-2024-minnesota-state-fair-foods/', 'displayLink': 'www.minnesotamonthly.com', 'snippet': \"Aug 23, 2024 ... ... hot, crisp, perfectly done deep-fried ranch). I've been describing it as a cream cheese wonton, but it's more like a deep fried ravioli. The\\xa0...\", 'htmlSnippet': 'Aug 23, 2024 <b>...</b> ... <b>hot</b>, crisp, perfectly done deep-fried ranch). I&#39;ve been describing it as a <b>cream cheese</b> wonton, but it&#39;s more like a deep fried ravioli. The&nbsp;...', 'formattedUrl': 'https://www.minnesotamonthly.com/.../review-new-2024-minnesota-state-f...', 'htmlFormattedUrl': 'https://www.minnesotamonthly.com/.../review-new-2024-minnesota-state-f...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRRVhFoeEdP9Z-x4R0dCKpR2HatYYeQZ1whkNBgfKQbpx4PlULQPh7wZLAh&s', 'width': '194', 'height': '259'}], 'imageobject': [{'url': '//cdn.minnesotamonthly.com/wp-content/uploads/sites/85/2019/03/MNMO_logos_WPnewspaper_LogoBlack-Upload.png'}, {'width': '1038', 'url': 'https://cdn.minnesotamonthly.com/wp-content/uploads/sites/85/2024/08/Screenshot-2024-08-23-at-8.50.38\\u202fAM.png', 'height': '920'}], 'person': [{'name': 'Jason DeRusha'}], 'organization': [{'name': 'Minnesota Monthly'}], 'metatags': [{'og:image': 'https://cdn.minnesotamonthly.com/wp-content/uploads/sites/85/2024/08/Screenshot-2024-08-23-at-8.50.38\\u202fAM.png', 'og:type': 'article', 'article:published_time': '2024-08-23T13:49:35+00:00', 'og:image:width': '1038', 'twitter:card': 'summary_large_image', 'og:site_name': 'Minnesota Monthly', 'tec-api-origin': 'https://www.minnesotamonthly.com', 'og:title': 'REVIEW: New 2024 Minnesota State Fair Foods | Minnesota Monthly', 'og:image:height': '920', 'twitter:label1': 'Written by', 'twitter:label2': 'Est. reading time', 'og:description': \"Food Editor Jason DeRusha and our team of tasters worked through the new fair foods. Here's what we loved, liked, and would skip next time.\", 'twitter:creator': '@DeRushaJ', 'article:author': 'TheJasonDeRusha', 'twitter:data1': 'Jason DeRusha', 'twitter:data2': '17 minutes', 'fb:app_id': '573304470912010', 'article:modified_time': '2024-08-29T16:18:58+00:00', 'viewport': 'width=device-width, initial-scale=1.0', 'og:locale': 'en_US', 'fb:admins': '100085565980029', 'og:url': 'https://www.minnesotamonthly.com/featured/homepage-feature/review-new-2024-minnesota-state-fair-foods/', 'tec-api-version': 'v1'}], 'cse_image': [{'src': 'https://cdn.minnesotamonthly.com/wp-content/uploads/sites/85/2024/08/IMG_9826-scaled.jpg'}], 'hatomfeed': [{}], 'article': [{'datemodified': '2024-08-29T11:18:58-05:00', 'headline': 'REVIEW: New 2024 Minnesota State Fair Foods', 'datepublished': '2024-08-23T08:49:35-05:00'}]}}, {'kind': 'customsearch#result', 'title': 'Food Lovers - Des Moines - Reviews from us, the customers. | So, I ...', 'htmlTitle': '<b>Food</b> Lovers - Des Moines - Reviews from <b>us</b>, the customers. | So, I ...', 'link': 'https://m.facebook.com/groups/541108569267853/posts/8725057670872861/', 'displayLink': 'm.facebook.com', 'snippet': \"Sep 11, 2024 ... So, I'm horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey's for dinner tonight. HOLY. COW.\", 'htmlSnippet': 'Sep 11, 2024 <b>...</b> So, I&#39;m horrible at writing these things-but I got the bacon <b>jalapeño popper</b> pizza from Casey&#39;s for dinner tonight. HOLY. COW.', 'formattedUrl': 'https://m.facebook.com/groups/541108569267853/.../8725057670872861/', 'htmlFormattedUrl': 'https://m.facebook.com/groups/541108569267853/.../8725057670872861/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQirUITaCli5-EXNdtsxm0Wn8RvIaxG0cqvh5R9RAf37DLAbmHkpWYQiC4&s', 'width': '300', 'height': '168'}], 'metatags': [{'apple-itunes-app': 'app-id=284882215, app-argument=fb://group/?id=541108569267853', 'og:image': 'https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=10160764998509541', 'twitter:card': 'summary', 'theme-color': '#FFFFFF', 'al:android:package': 'com.facebook.katana', 'al:ios:url': 'fb://group/?id=541108569267853', 'og:description': 'So, I’m horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey’s for dinner tonight.  HOLY. COW.  The jalapeños have the seeds so they do have kick, but they’re pickled...', 'al:ios:app_store_id': '284882215', 'twitter:image': 'https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=10160764998509541', 'twitter:image:alt': 'Food Lovers - Des Moines - Reviews from us, the customers. | So, I’m horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey’s for dinner tonight | Facebook', 'twitter:site': '@facebookapp', 'application-name': 'Facebook Groups', 'og:type': 'video.other', 'og:image:alt': 'Food Lovers - Des Moines - Reviews from us, the customers. | So, I’m horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey’s for dinner tonight | Facebook', 'twitter:title': 'Food Lovers - Des Moines - Reviews from us, the customers. | So, I’m...', 'al:ios:app_name': 'Facebook', 'application-url': 'https://www.facebook.com/groups/', 'og:title': 'Food Lovers - Des Moines - Reviews from us, the customers. | So, I’m horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey’s for dinner tonight | Facebook', 'google': 'notranslate', 'al:android:url': 'fb://group/541108569267853', 'referrer': 'origin-when-crossorigin', 'viewport': 'width=device-width, initial-scale=1', 'twitter:description': 'So, I’m horrible at writing these things-but I got the bacon jalapeño popper pizza from Casey’s for dinner tonight.  HOLY. COW.  The jalapeños have the seeds so they do have kick, but they’re pickled...', 'og:locale': 'en_US', 'fb-version': '430.0.0.0.46:2647', 'og:url': 'https://www.facebook.com/groups/541108569267853/posts/8725057670872861/', 'al:android:app_name': 'Facebook'}], 'cse_image': [{'src': 'https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=10160764998509541'}]}}, {'kind': 'customsearch#result', 'title': \"Our Menu | Bad Daddy's Burger Bar\", 'htmlTitle': 'Our Menu | Bad Daddy&#39;s Burger Bar', 'link': 'https://www.baddaddysburgerbar.com/our-menu', 'displayLink': 'www.baddaddysburgerbar.com', 'snippet': 'Our Buttermilk fried chicken breast with a spiced honey sauce, pickles and our house jalapeño ranch. Chicken Tenders. Hand-breaded crispy chicken tenders served\\xa0...', 'htmlSnippet': 'Our Buttermilk fried chicken breast with a <b>spiced</b> honey sauce, pickles and our <b>house jalapeño</b> ranch. Chicken Tenders. Hand-breaded crispy chicken tenders served&nbsp;...', 'formattedUrl': 'https://www.baddaddysburgerbar.com/our-menu', 'htmlFormattedUrl': 'https://www.baddaddysburgerbar.com/our-menu', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSNU-ax02O0gK5MdRa5xjt9ssIlzjPK5xPIpD2RZ6wyKqDsAVoWHwmEcBM&s', 'width': '225', 'height': '225'}], 'metatags': [{'twitter:title': \"Our Menu | Bad Daddy's Burger Bar\", 'og:type': 'website', 'msapplication-tap-highlight': 'no', 'apple-mobile-web-app-status-bar-style': 'black', 'viewport': 'width=device-width, viewport-fit=cover, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no', 'twitter:description': \"Bad Daddy's Burger Bar serves chef-driven burgers, chopped salads, artisan sandwiches, and local craft beers!\", 'og:title': \"Our Menu | Bad Daddy's Burger Bar\", 'mobile-web-app-capable': 'yes', 'title': \"Our Menu | Bad Daddy's Burger Bar\", 'og:url': 'https://www.baddaddysburgerbar.com/our-menu', 'og:description': \"Bad Daddy's Burger Bar serves chef-driven burgers, chopped salads, artisan sandwiches, and local craft beers!\", 'format-detection': 'telephone=no'}], 'cse_image': [{'src': 'https://dineengine.io/baddaddys/assets/abe20p8ntrswkwws'}]}}, {'kind': 'customsearch#result', 'title': 'At new Inver Grove Brewing, bar food, beer and cocktails are ...', 'htmlTitle': 'At new Inver Grove Brewing, bar <b>food</b>, beer and cocktails are ...', 'link': 'https://www.twincities.com/2019/11/27/inver-grove-brewing-review-bar-restaurant-food/', 'displayLink': 'www.twincities.com', 'snippet': 'Dec 1, 2019 ... ... spicy food, we decided on the jalapeno popper and the Dewey Thai. The former had all our favorite popper elements — spicy peppers, creamy\\xa0...', 'htmlSnippet': 'Dec 1, 2019 <b>...</b> ... <b>spicy food</b>, we decided on the <b>jalapeno popper</b> and the Dewey Thai. The former had all our favorite <b>popper</b> elements — <b>spicy</b> peppers, creamy&nbsp;...', 'formattedUrl': 'https://www.twincities.com/.../inver-grove-brewing-review-bar-restaurant-f...', 'htmlFormattedUrl': 'https://www.twincities.com/.../inver-grove-brewing-review-bar-restaurant-f...', 'pagemap': {'metatags': [{'application-name': 'twincities', 'og:image': 'https://www.twincities.com/wp-content/uploads/2019/11/IMG_3855.jpg?w=1024&h=739', 'og:type': 'article', 'article:published_time': '2019-11-27T11:26:58+00:00', 'og:image:width': '1024', 'twitter:card': 'summary_large_image', 'og:site_name': 'Twin Cities', 'msvalidate.01': '4B535F7EB2971D1FCBA5D1D3E3E292C3', 'og:title': 'At new Inver Grove Brewing, bar food, beer and cocktails are inventive and tasty', 'og:image:height': '739', 'msapplication-tileimage': 'https://www.twincities.com/wp-content/uploads/2017/06/cropped-faviconwhite-copy.jpg?w=270', 'fb:pages': '276896490404', 'og:description': 'In the suburbs, it can sometimes be hard to find independent restaurants serving food that’s even a little outside of the box. So I guess it shouldn’t have surprised me when we showed up at 7 p.m. …', 'twitter:image': 'https://www.twincities.com/wp-content/uploads/2019/11/IMG_3855.jpg?w=640', 'twitter:text:title': 'At new Inver Grove Brewing, bar food, beer and cocktails are inventive and tasty', 'twitter:site': '@pioneerpress', 'article:modified_time': '2019-12-01T20:18:25+00:00', 'viewport': 'width=device-width, initial-scale=1', 'og:locale': 'en_US', 'og:url': 'https://www.twincities.com/2019/11/27/inver-grove-brewing-review-bar-restaurant-food/'}]}}, {'kind': 'customsearch#result', 'title': 'Jack Mancuso (@chefcuso) • Instagram photos and videos', 'htmlTitle': 'Jack Mancuso (@chefcuso) • Instagram photos and videos', 'link': 'https://www.instagram.com/chefcuso/?hl=en', 'displayLink': 'www.instagram.com', 'snippet': 'Texas Twinkies Typically you use leftover brisket, but we used leftover shredded beef n lamb. Basically a leveled up jalapeño popper using leftover meat #\\xa0...', 'htmlSnippet': 'Texas Twinkies Typically you use leftover brisket, but we used leftover shredded beef n lamb. Basically a leveled up <b>jalapeño popper</b> using leftover meat #&nbsp;...', 'formattedUrl': 'https://www.instagram.com/chefcuso/?hl=en', 'htmlFormattedUrl': 'https://www.instagram.com/chefcuso/?hl=en', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRgWOzm0M2I0oZ5kn8O9NmB3oPzqInGqSb-602mMYEPgNj2B0U3EOZdmdo&s=0', 'width': '84', 'height': '150'}], 'xfn': [{}], 'metatags': [{'og:image': 'https://scontent-lax3-1.cdninstagram.com/v/t51.2885-19/278722372_685164402604210_5137614280743120519_n.jpg?stp=dst-jpg_s100x100&_nc_cat=1&ccb=1-7&_nc_sid=bf7eb4&_nc_ohc=h2rn4yftzU4Q7kNvgHVtZMB&_nc_zt=24&_nc_ht=scontent-lax3-1.cdninstagram.com&oh=00_AYChCzA8iPx7HjB9JlQIWBCCdKpIYI6YFpaZrHTN3xpZxQ&oe=6717B042', 'theme-color': '#FFFFFF', 'og:type': 'profile', 'al:ios:app_name': 'Instagram', 'og:title': 'Jack Mancuso (@chefcuso) • Instagram photos and videos', 'al:android:package': 'com.instagram.android', 'bingbot': 'noarchive', 'al:ios:url': 'instagram://user?username=chefcuso', 'color-scheme': 'light', 'og:description': '1M Followers, 311 Following, 1,209 Posts - See Instagram photos and videos from Jack Mancuso (@chefcuso)', 'al:ios:app_store_id': '389801252', 'al:android:url': 'https://instagram.com/_u/chefcuso/', 'apple-mobile-web-app-status-bar-style': 'default', 'viewport': 'width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, viewport-fit=cover', 'mobile-web-app-capable': 'yes', 'og:url': 'https://www.instagram.com/chefcuso/', 'al:android:app_name': 'Instagram'}], 'cse_image': [{'src': 'https://lookaside.instagram.com/seo/google_widget/crawler/?media_id=2930194255828524673'}]}}, {'kind': 'customsearch#result', 'title': \"Crispy Baked Jalapeno Poppers | Don't Go Bacon My Heart\", 'htmlTitle': 'Crispy Baked <b>Jalapeno Poppers</b> | Don&#39;t Go Bacon My Heart', 'link': 'https://www.dontgobaconmyheart.co.uk/baked-jalapeno-poppers/', 'displayLink': 'www.dontgobaconmyheart.co.uk', 'snippet': \"Jan 15, 2018 ... For a long time when making jalapeño poppers I'd do the whole cream cheese stuffed jalapeños, topped with cheddar cheese then wrap with bacon.\", 'htmlSnippet': 'Jan 15, 2018 <b>...</b> For a long time when making <b>jalapeño poppers</b> I&#39;d do the whole cream <b>cheese</b> stuffed jalapeños, topped with cheddar <b>cheese</b> then wrap with bacon.', 'formattedUrl': 'https://www.dontgobaconmyheart.co.uk/baked-jalapeno-poppers/', 'htmlFormattedUrl': 'https://www.dontgobaconmyheart.co.uk/baked-<b>jalapeno</b>-<b>popper</b>s/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSanFEYJa29gjxobhYR8YdsBIZi47ULotzhXswuh8ua5oVr_VR6sO7EF34p&s', 'width': '220', 'height': '229'}], 'metatags': [{'p:domain_verify': '6d1bfebe001e6e38eb1cbb261f699b06', 'og:image': 'https://www.dontgobaconmyheart.co.uk/wp-content/uploads/2017/05/crispy-baked-jalapeno-poppers-12.jpg', 'article:published_time': '2018-01-15T20:25:20+00:00', 'twitter:card': 'summary_large_image', 'og:image:width': '1966', 'og:site_name': \"Don't Go Bacon My Heart\", 'twitter:label1': 'Written by', 'twitter:label2': 'Est. reading time', 'slick:category': 'finger-food:Finger Food;recipes:Recipes by Category', 'msapplication-tileimage': 'https://www.dontgobaconmyheart.co.uk/wp-content/uploads/2017/10/cropped-tab-logo-dont-go-bacon-my-heart-270x270.png', 'og:description': \"Juicy\\xa0jalapeño\\xa0on bottom, with a loaded\\xa0centre\\xa0and a parmesan crunch on top, once you try these Crispy Baked Jalapeno Poppers you'll never look back.\", 'twitter:image': 'https://www.dontgobaconmyheart.co.uk/wp-content/uploads/2017/05/crispy-baked-jalapeno-poppers-12.jpg', 'twitter:data1': 'Chris Collins', 'twitter:data2': '9 minutes', 'article:modified_time': '2020-01-12T19:55:08+00:00', 'slick:featured_image': 'https://www.dontgobaconmyheart.co.uk/wp-content/uploads/2017/05/crispy-baked-jalapeno-poppers-12.jpg', 'og:type': 'article', 'twitter:title': \"Crispy Baked Jalapeno Poppers | Don't Go Bacon My Heart\", 'author': 'Chris Collins', 'og:title': \"Crispy Baked Jalapeno Poppers | Don't Go Bacon My Heart\", 'og:image:height': '2048', 'slick:wpversion': '1.4.4', 'og:updated_time': '2020-01-12T19:55:08+00:00', 'slick:group': 'post', 'pinterest-rich-pin': 'false', 'viewport': 'width=device-width, initial-scale=1', 'twitter:description': \"Juicy\\xa0jalapeño\\xa0on bottom, with a loaded\\xa0centre\\xa0and a parmesan crunch on top, once you try these Crispy Baked Jalapeno Poppers you'll never look back.\", 'slick:wppostid': '1820', 'og:locale': 'en_US', 'og:url': 'https://www.dontgobaconmyheart.co.uk/baked-jalapeno-poppers/'}], 'videoobject': [{'contenturl': 'https://content.jwplatform.com/videos/wL3RTlfZ.mp4', 'uploaddate': '2021-11-20T17:30:41.000Z', 'name': 'Crispy Baked Jalapeno Poppers', 'description': 'Juicy jalapeño on bottom, with a loaded centre and a parmesan crunch on top, once you try this way of baking jalapeño poppers you’ll never look back. These Crispy Baked Jalapeño Poppers...', 'thumbnailurl': 'https://content.jwplatform.com/thumbs/wL3RTlfZ-720.jpg'}], 'cse_image': [{'src': 'https://www.dontgobaconmyheart.co.uk/wp-content/uploads/2017/05/crispy-baked-jalapeno-poppers-12.jpg'}], 'hatomfeed': [{}]}}, {'kind': 'customsearch#result', 'title': 'Jack in the Box underrated? : r/fastfood', 'htmlTitle': 'Jack <b>in the</b> Box underrated? : r/fastfood', 'link': 'https://www.reddit.com/r/fastfood/comments/hw6y02/jack_in_the_box_underrated/', 'displayLink': 'www.reddit.com', 'snippet': 'Jul 23, 2020 ... I grew in love with the spicy chicken sandwich w/ cheese and the jalapeno poppers with their buttermilk ranch. Sometimes I even choose Jack\\xa0...', 'htmlSnippet': 'Jul 23, 2020 <b>...</b> I grew in love with the <b>spicy</b> chicken sandwich w/ <b>cheese</b> and the <b>jalapeno poppers</b> with their buttermilk ranch. Sometimes I even choose Jack&nbsp;...', 'formattedUrl': 'https://www.reddit.com/r/fastfood/comments/.../jack_in_the_box_underrate...', 'htmlFormattedUrl': 'https://www.reddit.com/r/fastfood/comments/.../jack_in_the_box_underrate...', 'pagemap': {'metatags': [{'og:image': 'https://share.redd.it/preview/post/hw6y02', 'theme-color': '#000000', 'og:image:width': '1200', 'og:type': 'website', 'og:image:alt': 'An image containing a preview of the post', 'twitter:card': 'summary_large_image', 'twitter:title': 'r/fastfood on Reddit: Jack in the Box underrated?', 'og:site_name': 'Reddit', 'og:title': 'r/fastfood on Reddit: Jack in the Box underrated?', 'og:image:height': '630', 'msapplication-navbutton-color': '#000000', 'og:description': 'Posted by u/JaydenOdegaard - 82 votes and 62 comments', 'twitter:image': 'https://share.redd.it/preview/post/hw6y02', 'apple-mobile-web-app-status-bar-style': 'black', 'twitter:site': '@reddit', 'viewport': 'width=device-width, initial-scale=1, viewport-fit=cover', 'apple-mobile-web-app-capable': 'yes', 'og:ttl': '600', 'og:url': 'https://www.reddit.com/r/fastfood/comments/hw6y02/jack_in_the_box_underrated/'}], 'cse_image': [{'src': 'https://share.redd.it/preview/post/hw6y02'}]}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ8soO5Mbp6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g3zqbv5LboyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d077fbc-4784-4de7-aa73-f3238e71e490",
        "collapsed": true,
        "id": "neOAG4KgbeRQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n",
            "{'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - american food', 'totalResults': '6650000000', 'searchTerms': 'american food', 'count': 10, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}], 'nextPage': [{'title': 'Google Custom Search - american food', 'totalResults': '6650000000', 'searchTerms': 'american food', 'count': 10, 'startIndex': 11, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f6f517e4450dc4281'}]}, 'context': {'title': 'FoodSearch'}, 'searchInformation': {'searchTime': 0.440453, 'formattedSearchTime': '0.44', 'totalResults': '6650000000', 'formattedTotalResults': '6,650,000,000'}, 'items': [{'kind': 'customsearch#result', 'title': 'American food: The 50 greatest dishes | CNN', 'htmlTitle': '<b>American food</b>: The 50 greatest dishes | CNN', 'link': 'https://www.cnn.com/travel/article/american-food-dishes/index.html', 'displayLink': 'www.cnn.com', 'snippet': \"To celebrate its endless culinary creativity, we're throwing our list of 50 most delicious American food items at you. We know you're going to want to throw\\xa0...\", 'htmlSnippet': 'To celebrate its endless culinary creativity, we&#39;re throwing our list of 50 most delicious <b>American food</b> items at you. We know you&#39;re going to want to throw&nbsp;...', 'formattedUrl': 'https://www.cnn.com/travel/article/american-food-dishes/index.html', 'htmlFormattedUrl': 'https://www.cnn.com/travel/article/<b>american</b>-<b>food</b>-dishes/index.html', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSUeZrPJy_5Pd-8qn3SFF-pMCFTJCT_-czKZnGGW7WFrfiyTbgzzWpbphM&s', 'width': '300', 'height': '168'}], 'metatags': [{'og:image': 'https://media.cnn.com/api/v1/images/stellar/prod/170203114333-cheeseburger.jpg?q=x_0,y_156,h_1688,w_3000,c_crop/w_800', 'twitter:title': 'American food: The 50 greatest dishes | CNN', 'og:type': 'article', 'twitter:card': 'summary_large_image', 'article:published_time': '2017-07-12T11:54:43Z', 'og:site_name': 'CNN', 'author': 'Dana Joseph', 'og:title': 'American food: The 50 greatest dishes | CNN', 'meta-section': 'travel', 'type': 'article', 'og:description': 'We have chips and grits. We have ribs and wings. We even have a salad. Do you have the appetite for the 50 greatest dishes of American food?', 'twitter:image': 'https://media.cnn.com/api/v1/images/stellar/prod/170203114333-cheeseburger.jpg?q=x_0,y_156,h_1688,w_3000,c_crop/w_800', 'article:publisher': 'https://www.facebook.com/CNN', 'article:tag': 'food and drink, beef, business and industry sectors, business, economy and trade, consumer products, food products, fruits and vegetables, kinds of foods and beverages, meat products, vegetables, dairy products, asian food, cooking and entertaining, international cuisine, food and beverage industry, leisure and lifestyle, north america, pasta and noodles, pork, restaurant and food service industry, restaurant industry, restaurants, southeastern united states, southwestern united states, texas, united states, virginia, beans and legumes, california, san francisco, condiments, los angeles, minority and ethnic groups, native americans, packaged foods, snack-food, society, florida, mexican and south american food, soup, chicago, chile, festivals, fruit, illinois, latin america, midwestern united states, new mexico, south america, sweets and desserts, usa travel guide, continents and regions, the americas', 'fb:app_id': '80401312489', 'twitter:site': '@cnntravel', 'article:modified_time': '2021-01-17T03:18:11Z', 'viewport': 'width=device-width,initial-scale=1,shrink-to-fit=no', 'twitter:description': 'We have chips and grits. We have ribs and wings. We even have a salad. Do you have the appetite for the 50 greatest dishes of American food?', 'template_type': 'article_leaf', 'meta-branding': 'life-but-better-diet', 'theme': 'travel', 'og:url': 'https://www.cnn.com/travel/article/american-food-dishes/index.html'}], 'cse_image': [{'src': 'https://media.cnn.com/api/v1/images/stellar/prod/170203114333-cheeseburger.jpg?q=x_0,y_156,h_1688,w_3000,c_crop/w_800'}]}}, {'kind': 'customsearch#result', 'title': 'List of American foods - Wikipedia', 'htmlTitle': 'List of <b>American foods</b> - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/List_of_American_foods', 'displayLink': 'en.wikipedia.org', 'snippet': '1.1 Breads and baked goods 1.2 Cheese and dairy 1.3 Breakfast 1.4 Vegetables and salads 1.5 Main dishes 1.6 Soups and stews 1.7 Desserts 1.8 Rice dishes 1.9\\xa0...', 'htmlSnippet': '1.1 Breads and baked goods 1.2 Cheese and dairy 1.3 Breakfast 1.4 Vegetables and salads 1.5 Main dishes 1.6 Soups and stews 1.7 Desserts 1.8 Rice dishes 1.9&nbsp;...', 'formattedUrl': 'https://en.wikipedia.org/wiki/List_of_American_foods', 'htmlFormattedUrl': 'https://en.wikipedia.org/wiki/List_of_<b>American</b>_<b>food</b>s', 'pagemap': {'metatags': [{'referrer': 'origin', 'og:image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Apple_pie.jpg/1200px-Apple_pie.jpg', 'theme-color': '#eaecf0', 'og:image:width': '1200', 'og:type': 'website', 'viewport': 'width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=0.25, maximum-scale=5.0', 'og:title': 'List of American foods - Wikipedia', 'og:image:height': '822', 'format-detection': 'telephone=no'}]}}, {'kind': 'customsearch#result', 'title': 'American Foods Group', 'htmlTitle': '<b>American Foods</b> Group', 'link': 'https://www.americanfoodsgroup.com/', 'displayLink': 'www.americanfoodsgroup.com', 'snippet': 'We are privately held and family-run for over 70 years. We employ more than 4,500 workers across the country, and we work together as a family to provide safe,\\xa0...', 'htmlSnippet': 'We are privately held and family-run for over 70 years. We employ more than 4,500 workers across the country, and we work together as a family to provide safe,&nbsp;...', 'formattedUrl': 'https://www.americanfoodsgroup.com/', 'htmlFormattedUrl': 'https://www.<b>americanfood</b>sgroup.com/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrrRA7Z1ngpev3fDCD22_jgmzymF5YTbfI78-aY6sRFtjy3ZcCRUSKNXxI&s', 'width': '238', 'height': '212'}], 'metatags': [{'application-name': 'American Foods Group', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': 'width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0', 'html': 'apple-mobile-web-app-title American Foods Group', 'og:url': 'https%3A%2F%2Fwww.americanfoodsgroup.com%2F%3F66e2708271b87'}], 'cse_image': [{'src': 'https://www.americanfoodsgroup.com/img/cache/content_regions/ContentRegions/108/image/61027-cd63be20c59fc1691a577d97144621ae'}]}}, {'kind': 'customsearch#result', 'title': 'What are some \"American\" foods ? : r/AskAnAmerican', 'htmlTitle': 'What are some &quot;<b>American</b>&quot; <b>foods</b> ? : r/AskAnAmerican', 'link': 'https://www.reddit.com/r/AskAnAmerican/comments/1ag93wl/what_are_some_american_foods/', 'displayLink': 'www.reddit.com', 'snippet': 'Feb 1, 2024 ... I know about Steaks in Texas and Pizza in NYC. But for me it feels like most of these are imported by the nature of the US as a young country based of\\xa0...', 'htmlSnippet': 'Feb 1, 2024 <b>...</b> I know about Steaks in Texas and Pizza in NYC. But for me it feels like most of these are imported by the nature of the <b>US</b> as a young country based of&nbsp;...', 'formattedUrl': 'https://www.reddit.com/.../AskAnAmerican/.../what_are_some_american_fo...', 'htmlFormattedUrl': 'https://www.reddit.com/.../AskAn<b>American</b>/.../what_are_some_<b>american</b>_<b>fo</b>...', 'pagemap': {'metatags': [{'og:image': 'https://share.redd.it/preview/post/1ag93wl', 'theme-color': '#000000', 'og:image:width': '1200', 'og:type': 'website', 'og:image:alt': 'An image containing a preview of the post', 'twitter:card': 'summary_large_image', 'twitter:title': 'r/AskAnAmerican on Reddit: What are some \"American\" foods ?', 'og:site_name': 'Reddit', 'og:title': 'r/AskAnAmerican on Reddit: What are some \"American\" foods ?', 'og:image:height': '630', 'msapplication-navbutton-color': '#000000', 'og:description': 'Posted by u/TheZerbio - 98 votes and 515 comments', 'twitter:image': 'https://share.redd.it/preview/post/1ag93wl', 'apple-mobile-web-app-status-bar-style': 'black', 'twitter:site': '@reddit', 'viewport': 'width=device-width, initial-scale=1, viewport-fit=cover', 'apple-mobile-web-app-capable': 'yes', 'og:ttl': '600', 'og:url': 'https://www.reddit.com/r/AskAnAmerican/comments/1ag93wl/what_are_some_american_foods/'}], 'cse_image': [{'src': 'https://share.redd.it/preview/post/1ag93wl'}]}}, {'kind': 'customsearch#result', 'title': 'NAFSA | Native American Food Sovereignty Alliance | Restoring ...', 'htmlTitle': 'NAFSA | Native <b>American Food</b> Sovereignty Alliance | Restoring ...', 'link': 'https://nativefoodalliance.org/', 'displayLink': 'nativefoodalliance.org', 'snippet': 'The Native American Food Sovereignty Alliance (NAFSA) supports Native communities nationally with advocacy, education, and networking as they revitalize their\\xa0...', 'htmlSnippet': 'The Native <b>American Food</b> Sovereignty Alliance (NAFSA) supports Native communities nationally with advocacy, education, and networking as they revitalize their&nbsp;...', 'formattedUrl': 'https://nativefoodalliance.org/', 'htmlFormattedUrl': 'https://native<b>food</b>alliance.org/', 'pagemap': {'metatags': [{'tec-api-origin': 'https://nativefoodalliance.org', 'viewport': 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0', 'msapplication-tileimage': 'https://nativefoodalliance.org/wp-content/uploads/2020/03/cropped-116078-200-270x270.png', 'tec-api-version': 'v1'}]}}, {'kind': 'customsearch#result', 'title': 'A&W Restaurants | A&W® All American Food', 'htmlTitle': 'A&amp;W Restaurants | A&amp;W® All <b>American Food</b>', 'link': 'https://awrestaurants.com/', 'displayLink': 'awrestaurants.com', 'snippet': \"Looking for some hometown goodness? Take a look at what's going on at your local A&W, stop in and lets us know how we're doing!\", 'htmlSnippet': 'Looking for some hometown goodness? Take a look at what&#39;s going on at your local A&amp;W, stop in and lets <b>us</b> know how we&#39;re doing!', 'formattedUrl': 'https://awrestaurants.com/', 'htmlFormattedUrl': 'https://awrestaurants.com/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMyhiPi3m1c4Xd3yW22I__IwfXhh8-JRGee7_b0NW8Twkp9lHdpsiuxgRP&s', 'width': '275', 'height': '183'}], 'metatags': [{'og:type': 'website', 'twitter:title': 'A&W Restaurants', 'viewport': 'width=device-width, initial-scale=1, shrink-to-fit=no', 'twitter:url': 'https://awrestaurants.com/', 'og:title': 'A&W Restaurants', 'title': 'A&W Restaurants', 'og:url': 'https://awrestaurants.com/'}], 'cse_image': [{'src': 'https://awrestaurants.com/sites/default/files/Capn-Crunch-Shake-Header-2024-MOBILE.jpg'}], 'Document': [{'num_replies': '0', 'title': 'Home'}]}}, {'kind': 'customsearch#result', 'title': 'U.S. Food and Drug Administration', 'htmlTitle': '<b>U.S. Food</b> and Drug Administration', 'link': 'https://www.fda.gov/', 'displayLink': 'www.fda.gov', 'snippet': 'The FDA is responsible for protecting the public health by ensuring the safety, efficacy, and security of human and veterinary drugs, biological products,\\xa0...', 'htmlSnippet': 'The FDA is responsible for protecting the public health by ensuring the safety, efficacy, and security of human and veterinary drugs, biological products,&nbsp;...', 'formattedUrl': 'https://www.fda.gov/', 'htmlFormattedUrl': 'https://www.fda.gov/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8PwCSyBEqPFXCQR3_z9KNFB4UkPiqQPUzKE0Ck8igX_6L91A2U02b23k&s', 'width': '300', 'height': '168'}], 'metatags': [{'og:image': 'https://www.fda.gov/themes/custom/preview/img/FDA-Social-Graphic.png', 'og:type': 'website', 'article:published_time': 'Wed, 09/11/2024 - 11:39', 'twitter:card': 'summary_large_image', 'twitter:title': 'U.S. Food and Drug Administration', 'og:site_name': 'U.S. Food and Drug Administration', 'handheldfriendly': 'true', 'dcterms.title': 'U.S. Food and Drug Administration', 'og:title': 'U.S. Food and Drug Administration', 'dcterms.creator': 'Office of the Commissioner', 'dcterms.description': \"The FDA is responsible for protecting the public health by ensuring the safety, efficacy, and security of human and veterinary drugs, biological products, and medical devices; and by ensuring the safety of our nation's food supply, cosmetics, and products that products that emit radiation\", 'og:updated_time': 'Mon, 09/09/2024 - 10:00', 'og:description': 'The FDA is responsible for protecting and promoting the public health', 'twitter:creator': '@US_FDA', 'twitter:image': 'https://www.fda.gov/themes/custom/preview/img/FDA-Social-Graphic.png', 'twitter:site': '@US_FDA', 'dcterms.source': 'FDA', 'article:modified_time': 'Mon, 09/09/2024 - 10:00', 'viewport': 'width=device-width, initial-scale=1.0', 'dcterms.publisher': 'FDA', 'twitter:description': 'The FDA is responsible for protecting and promoting the public health', 'mobileoptimized': 'width', 'og:url': 'https://www.fda.gov/home'}], 'cse_image': [{'src': 'https://www.fda.gov/themes/custom/preview/img/FDA-Social-Graphic.png'}]}}, {'kind': 'customsearch#result', 'title': '\"What\\'s a muzzadell?\" Exploring Italian American Food Vocabulary ...', 'htmlTitle': '&quot;What&#39;s a muzzadell?&quot; Exploring Italian <b>American Food</b> Vocabulary ...', 'link': 'https://www.jplinguistics.com/italian-language-and-culture/whats-a-muzzadell-exploring-italian-american-food-vocabulary', 'displayLink': 'www.jplinguistics.com', 'snippet': \"Aug 13, 2020 ... Differences Between Italian American and Standard Italian Words · Brosciutt' : prosciutto · Gabagool : capacolla · Fajool (think “pasta fajool”)\\xa0...\", 'htmlSnippet': 'Aug 13, 2020 <b>...</b> Differences Between Italian <b>American</b> and Standard Italian Words &middot; Brosciutt&#39; : prosciutto &middot; Gabagool : capacolla &middot; Fajool (think “pasta fajool”)&nbsp;...', 'formattedUrl': 'https://www.jplinguistics.com/.../whats-a-muzzadell-exploring-italian-ameri...', 'htmlFormattedUrl': 'https://www.jplinguistics.com/.../whats-a-muzzadell-exploring-italian-<b>ameri</b>...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ665hX7MU23ukD-QuI7aPfkm2dN0LVUbz8z1xQLHtmLvANRmvjsqt_Jwo&s', 'width': '264', 'height': '191'}], 'metatags': [{'og:street-address': '154 Grand Street', 'og:region': 'NY', 'og:image': 'http://static1.squarespace.com/static/516411ffe4b032c4bb3b62e3/5797ef73d482e91ef6c9f45a/5f340eb47f67934d5dbe0da2/1724359396154/jez-timms-6JQMjhqpVhE-unsplash.jpg?format=1500w', 'og:type': 'article', 'og:image:width': '1500', 'twitter:title': '\"What\\'s a muzzadell?\" Exploring Italian American Food Vocabulary — JP Linguistics - French, Italian, Spanish Classes in NYC', 'twitter:card': 'summary', 'og:site_name': 'JP Linguistics - French, Italian, Spanish Classes in NYC', 'twitter:url': 'https://www.jplinguistics.com/italian-language-and-culture/whats-a-muzzadell-exploring-italian-american-food-vocabulary', 'og:locality': 'New York', 'og:title': '\"What\\'s a muzzadell?\" Exploring Italian American Food Vocabulary — JP Linguistics - French, Italian, Spanish Classes in NYC', 'og:image:height': '1088', 'og:latitude': '40.7202344', 'og:description': 'Have you ever wondered why the Italian at your deli is different from the Italian in your textbook?', 'twitter:image': 'http://static1.squarespace.com/static/516411ffe4b032c4bb3b62e3/5797ef73d482e91ef6c9f45a/5f340eb47f67934d5dbe0da2/1724359396154/jez-timms-6JQMjhqpVhE-unsplash.jpg?format=1500w', 'og:longitude': '-73.9986357', 'og:postal-code': '10013', 'viewport': 'width=device-width, initial-scale=1', 'twitter:description': 'Have you ever wondered why the Italian at your deli is different from the Italian in your textbook?', 'og:url': 'https://www.jplinguistics.com/italian-language-and-culture/whats-a-muzzadell-exploring-italian-american-food-vocabulary', 'og:country-name': 'United States'}], 'cse_image': [{'src': 'http://static1.squarespace.com/static/516411ffe4b032c4bb3b62e3/5797ef73d482e91ef6c9f45a/5f340eb47f67934d5dbe0da2/1724359396154/jez-timms-6JQMjhqpVhE-unsplash.jpg?format=1500w'}]}}, {'kind': 'customsearch#result', 'title': 'Hugh Hayden: American Food | Exhibitions | Lisson Gallery', 'htmlTitle': 'Hugh Hayden: <b>American Food</b> | Exhibitions | Lisson Gallery', 'link': 'https://www.lissongallery.com/exhibitions/hugh-hayden-american-food', 'displayLink': 'www.lissongallery.com', 'snippet': 'Jul 31, 2020 ... Hayden considers Southern cooking the first uniquely American cuisine, having originated in kitchens run by the enslaved cooks, who infused\\xa0...', 'htmlSnippet': 'Jul 31, 2020 <b>...</b> Hayden considers Southern cooking the first uniquely <b>American</b> cuisine, having originated in kitchens run by the enslaved cooks, who infused&nbsp;...', 'formattedUrl': 'https://www.lissongallery.com/exhibitions/hugh-hayden-american-food', 'htmlFormattedUrl': 'https://www.lissongallery.com/exhibitions/hugh-hayden-<b>american</b>-<b>food</b>', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbezz_JEOWfJOAq7IVJdJOgNiEEqRVaTJJjIsJrH49imLjN4XpSZJtZCQ&s', 'width': '256', 'height': '197'}], 'Event': [{'image': 'https://lisson-art.s3.amazonaws.com/uploads/attachment/image/body/16025/HAYD_WEBSITE2.jpg', 'endDate': '2020-07-31T00:00:00+01:00', 'eventStatus': 'http://schema.org/EventScheduled', 'name': 'Hugh Hayden: American Food', 'description': 'Lisson Gallery is pleased to present Hugh Hayden’', 'eventAttendanceMode': 'http://schema.org/OfflineEventAttendanceMode', 'startDate': '2020-03-12T00:00:00Z'}], 'metatags': [{'msapplication-tilecolor': '#ffc40d', 'msapplication-config': '/assets/favicon/browserconfig-08ce39baee428ddcd8acd29e7028a14efbc67e7619fc45b8dd5f9f79cff75651.xml', 'theme-color': '#ffffff', 'twitter:card': 'summary', 'twitter:title': 'Lisson Gallery', 'og:type': 'website', 'og:site_name': 'Lisson Gallery', 'twitter:url': 'https://www.lissongallery.com/exhibitions/hugh-hayden-american-food', 'csrf-param': 'authenticity_token', 'twitter:image': 'https://www.lissongallery.com/assets/social-b3100c6ecf91564964002168d110b313c39adbf49a1174b313f69e7c0340bc47.jpg', 'twitter:site': '@Lisson_Gallery', 'viewport': 'width=device-width, initial-scale=1', 'csrf-token': 'RaXT9Z2QLLmel1M7F5g7UI1mW5qp1avqRRQa5tRrv-jbwOVPB9GEOUOCjdKspDLLGCt9VrqGSKHdn4n0Vk-Vyg'}], 'cse_image': [{'src': 'https://lisson-art.s3.amazonaws.com/uploads/attachment/image/body/16025/HAYD_WEBSITE2.jpg'}]}}, {'kind': 'customsearch#result', 'title': 'American Food Equipment Company: AMFEC', 'htmlTitle': '<b>American Food</b> Equipment Company: AMFEC', 'link': 'https://amfec.com/', 'displayLink': 'amfec.com', 'snippet': 'Established in 1975, has earned and maintained a reputation in the American food industry of supplying high-quality equipment engineered to meet specific\\xa0...', 'htmlSnippet': 'Established in 1975, has earned and maintained a reputation in the <b>American food</b> industry of supplying high-quality equipment engineered to meet specific&nbsp;...', 'formattedUrl': 'https://amfec.com/', 'htmlFormattedUrl': 'https://amfec.com/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRrbjwo__Z1AzIIURHduwHxoDY5gVpnQh9yRKJ_ZK7mYyADJ1LYd9zE5nU&s', 'width': '281', 'height': '180'}], 'metatags': [{'og:image': '/wp-content/uploads/2019/06/AMFEC-Logo-Flat-1.png', 'theme-color': '#ffffff', 'og:image:width': '800', 'article:published_time': '2019-02-28T22:54:26-06:00', 'twitter:card': 'summary_large_image', 'og:site_name': 'AMFEC - American Food Equipment Company', 'twitter:label1': 'Written by', 'twitter:label2': 'Time to read', 'og:image:type': 'image/png', 'og:description': 'American Food Equipment Company: Est. in 1975, has earned and maintained a reputation in the food industry of supplying high-quality equipment.', 'og:image:secure_url': 'https://amfec.com/wp-content/uploads/2019/03/AMFEC-Logo-Flat.png', 'twitter:image': 'https://amfec.com/wp-content/uploads/2019/03/AMFEC-Logo-Flat.png', 'twitter:data1': 'root', 'twitter:data2': 'Less than a minute', 'article:modified_time': '2021-09-16T11:33:55-06:00', 'og:type': 'website', 'og:image:alt': 'American Food Equipment Company', 'twitter:title': 'AMFEC - American Food Equipment Company', 'og:title': 'AMFEC - American Food Equipment Company', 'og:image:height': '513', 'og:updated_time': '2021-09-16T11:33:55-06:00', 'viewport': 'width=device-width, initial-scale=1, maximum-scale=1', 'twitter:description': 'American Food Equipment Company: Est. in 1975, has earned and maintained a reputation in the food industry of supplying high-quality equipment.', 'og:locale': 'en_US', 'og:url': 'https://amfec.com/', 'format-detection': 'telephone=no'}], 'webpage': [{'maincontentofpage': 'Welcome to American Food Equipment CompanyAmerican Food Equipment Company - AMFEC - Established in 1975, has earned and maintained a reputation in the American food industry of supplying high-quali...'}], 'cse_image': [{'src': 'https://amfec.com/wp-content/uploads/2019/06/AMFEC-Logo-Flat-1.png'}]}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"YOUR_CSE_ID\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"YOUR_QUERY\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823cc559-7feb-4be6-e2ec-a72f15bc6deb",
        "collapsed": true,
        "id": "UvY9oREuDaRT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.12.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.16.0->google-api-python-client==1.12.1) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.12.1) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client==1.12.1) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.1) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mIaUlcKRbtwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pVZnKAfybm7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"YOUR_CSE_ID\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"YOUR_QUERY\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k4us7oY1Dboo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xkExVrcnbwA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((n)) Esempio API per effettuare query con l'API Custom Search di Google in Colab.\n",
        "\n",
        "!pip install google-api-python-client==1.12.1\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# La tua chiave API\n",
        "my_api_key = \"AIzaSyAhD7HDaAebkHH5_WdgozYyuEkGd1UptdU\"\n",
        "\n",
        "# ID del tuo motore di ricerca personalizzato\n",
        "my_cse_id = \"f6f517e4450dc4281\"\n",
        "\n",
        "# Crea un servizio Custom Search\n",
        "service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "\n",
        "# Effettua una query\n",
        "res = service.cse().list(q=\"american food\", cx=my_cse_id).execute()\n",
        "\n",
        "# Stampa i risultati\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "znWRbHKmDcLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/version\n",
        "!lsb_release -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6naIZf66n2p",
        "outputId": "0387d453-adf9-4bf3-8ddd-20e8a07f067d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux version 6.1.85+ (builder@faf02fc191fd) (Chromium OS 16.0_pre484197_p20230405-r10 clang version 16.0.0 (/var/tmp/portage/sys-devel/llvm-16.0_pre484197_p20230405-r10/work/llvm-16.0_pre484197_p20230405/clang 2916b99182752b1aece8cc4479d8d6a20b5e02da), LLD 16.0.0) #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.3 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempio di riduzione della capacità di una immagine jpg\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!pip install Pillow\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "image_path ='/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immaginiDaRidurre/philadelphia_formaggio_originale.jpg';\n",
        "image=Image.open(image_path);\n",
        "resized_image=image.resize((1200,750));\n",
        "resized_image.save('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immaginiDaRidurre/philadelphia_formaggio_originale1.jpg');\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFs7oqDtM9JU",
        "outputId": "fcba42df-9e24-4295-c20e-8b419f483d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_index(frase1, frase2):\n",
        "  \"\"\"\n",
        "  Calcola l'indice di somiglianza tra due frasi,\n",
        "  cioè il numero di parole in comune.\n",
        "  \"\"\"\n",
        "  parole1 = set(frase1.lower().split())\n",
        "  parole2 = set(frase2.lower().split())\n",
        "  parole_in_comune = parole1.intersection(parole2)\n",
        "  return len(parole_in_comune)\n",
        "\n",
        "# Esempio di utilizzo\n",
        "frase1 = \"BELGIoIOSO FRESH MOZZARELLA Ahckica Favoriii 03 SNACKING CHEESE E FRESHI Convenient 70 Caloric Suack BELGIOIOSI FRESH MOZZARELLA Culoneokack With Every Veat Haeud Fresh Mozzarella 1Ooz Snacking Cheese Bite Smile\"\n",
        "frase2 = \"BELGIoIOSO FRESH MOZZARELLA\"\n",
        "indice = similarity_index(frase1, frase2)\n",
        "print(f\"Le frasi hanno {indice} parole in comune.\")\n",
        "\n",
        "\"\"\"\n",
        "Use code with caution\n",
        "\n",
        "Spiegazione:\n",
        "\n",
        "    Funzione similarity_index:\n",
        "        Prende in input due frasi (frase1 e frase2).\n",
        "        Converte entrambe le frasi in minuscolo e le divide in parole usando .lower().split().\n",
        "        Crea due insiemi (set) dalle parole di ciascuna frase per rimuovere i duplicati.\n",
        "        Trova l'intersezione tra i due insiemi usando .intersection(), ottenendo le parole in comune.\n",
        "        Restituisce la lunghezza dell'insieme di parole in comune come indice di somiglianza.\n",
        "\n",
        "    Esempio di utilizzo:\n",
        "        Definisce due frasi di esempio.\n",
        "        Chiama la funzione similarity_index per calcolare l'indice di somiglianza.\n",
        "        Stampa il risultato, indicando quante parole sono in comune tra le due frasi.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "kq73wbdFt0mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import easyocr\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# main_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immagini/Walmart'  # Sostituisci con il percorso corretto\n",
        "\n",
        "main_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_demo03092024'  # Sostituisci con il percorso corretto\n",
        "\n",
        "\n",
        "reader = easyocr.Reader(['it','en'])  # Sostituisci 'it' con la lingua appropriata se necessario\n",
        "\n",
        "results = []\n",
        "\n",
        "for cartellaSecondaria in os.listdir(main_folder):\n",
        "    cartellaSecondaria_path = os.path.join(main_folder, cartellaSecondaria)\n",
        "    if os.path.isdir(cartellaSecondaria_path):\n",
        "        for sottoCartella in os.listdir(cartellaSecondaria_path):\n",
        "            sottoCartella_path = os.path.join(cartellaSecondaria_path, sottoCartella)\n",
        "            if os.path.isdir(sottoCartella_path):\n",
        "                for filename in os.listdir(sottoCartella_path):\n",
        "                    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        image_path = os.path.join(sottoCartella_path, filename)\n",
        "                        image = cv2.imread(image_path)\n",
        "\n",
        "                        # Ridimensiona l'immagine\n",
        "                        image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        # Migliora il contrasto\n",
        "                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "                        image = clahe.apply(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
        "\n",
        "                        # Rimozione del rumore (opzionale)\n",
        "                        image = cv2.fastNlMeansDenoising(image, None, 10, 7, 21)\n",
        "\n",
        "                        result = reader.readtext(image_path)\n",
        "                        extracted_text = \" \".join([detection[1] for detection in result])\n",
        "                        results.append({\n",
        "                            \"main_folder\": os.path.basename(main_folder),\n",
        "                            \"cartellaSecondaria\": cartellaSecondaria,\n",
        "                            \"sottoCartella\": sottoCartella,\n",
        "                            \"filename\": filename,\n",
        "                            \"text\": extracted_text\n",
        "                        })\n",
        "\n",
        "with open('appItalianSolvingImg_immagini.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "34pGuk67pyi7",
        "outputId": "9b140b9c-4799-488b-b11f-3bb154622326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'easyocr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-893b04d47225>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0measyocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Terzo Esempio TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "import easyocr\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"_1000\")\n",
        "\n",
        "# image_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immagini/Walmart/Amys/pasta'  # Sostituisci con il percorso corretto\n",
        "# image_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immagini/Walmart/Arheon/pasta'  # Sostituisci con il percorso corretto\n",
        "# image_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immagini/Walmart/Buitoni/pasta'  # Sostituisci con il percorso corretto\n",
        "image_folder = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/immagini/Walmart/BelGioioso/formaggio'  # Sostituisci con il percorso corretto\n",
        "reader = easyocr.Reader(['it','en'])  # Sostituisci 'it' con la lingua appropriata se necessario\n",
        "\n",
        "results = []\n",
        "print(\"_2000\")\n",
        "\n",
        "for filename in os.listdir(image_folder):\n",
        "    print(\"_3000\")\n",
        "    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "        print(\"_4000\")\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Ridimensiona l'immagine\n",
        "        image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Migliora il contrasto\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        image = clahe.apply(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
        "\n",
        "        # Rimozione del rumore (opzionale)\n",
        "        image = cv2.fastNlMeansDenoising(image, None, 10, 7, 21)\n",
        "\n",
        "        result = reader.readtext(image_path)\n",
        "        extracted_text = \" \".join([detection[1] for detection in result])  # Uniamo tutti i testi estratti\n",
        "        print(\"_5000\")\n",
        "        results.append({\n",
        "            \"filename\": filename,\n",
        "            \"text\": extracted_text\n",
        "        })\n",
        "print(\"_6000\")\n",
        "\n",
        "with open('appItalianSolvingImg_immagini.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"_7000\")\n"
      ],
      "metadata": {
        "id": "FTrQ2uyFcdCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "cfe8b163-cae4-41a3-8c65-3a751338e4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'easyocr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4fe18e43bc30>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (Terzo Esempio TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0measyocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Secondo Esempio TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "!pip install easyocr torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# (Secondo Esempio TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "import easyocr\n",
        "import cv2\n",
        "\n",
        "reader = easyocr.Reader(['it'], gpu=False) # Abilita la GPU\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"_1000\")\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_tecnicaTextRecognition/TestPrototipoImmaginiStrutturate/augmented_image_40.jpg'\n",
        "print(\"_2000\")\n",
        "\n",
        "# (Riduzione dimensione immagine)\n",
        "immagine = cv2.imread(data_dir)\n",
        "immagine_ridotta = cv2.resize(immagine, (0, 0), fx=0.5, fy=0.5) # Riduce del 50%\n",
        "print(\"_3000\")\n",
        "\n",
        "# (Stima del Tempo)\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# immagine = cv2.imread(data_dir)\n",
        "result = reader.readtext(immagine_ridotta)\n",
        "\n",
        "for detection in result:\n",
        "    text = detection[1]\n",
        "    print(text)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Tempo di elaborazione:\", end_time - start_time, \"secondi\")\n"
      ],
      "metadata": {
        "id": "6WapgQsQYtny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swSCY4lgcb5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# (Primo Esempio, prima parte, TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "!pip install easyocr\n",
        "\n",
        "import easyocr\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"_1000\")\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_tecnicaTextRecognition/WalmartImmaginiNonStrutturate/2b8b45e5-06ae-466d-92b1-17e91d845306.a9abe23217a23a46293d3af0b3f7d858.jpeg'\n",
        "print(\"_2000\")\n",
        "\n",
        "reader = easyocr.Reader(['en']) # Specifica la lingua italiana\n",
        "\n",
        "# Sostituisci 'percorso/della/tua/immagine.jpg' con il percorso effettivo\n",
        "immagine = cv2.imread(data_dir)\n",
        "\n",
        "result = reader.readtext(immagine)\n",
        "\n",
        "for detection in result:\n",
        "    text = detection[1]\n",
        "    print(text)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VmJDyCOYFY5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Primo Esempio, seconda parte, TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_tecnicaTextRecognition/WalmartImmaginiNonStrutturate/2b8b45e5-06ae-466d-92b1-17e91d845306.a9abe23217a23a46293d3af0b3f7d858.jpeg'\n",
        "print(\"_2000\")\n",
        "\n",
        "reader = easyocr.Reader(['en']) # Specifica la lingua italiana\n",
        "\n",
        "# Sostituisci 'percorso/della/tua/immagine.jpg' con il percorso effettivo\n",
        "immagine = cv2.imread(data_dir)\n",
        "\n",
        "result = reader.readtext(immagine)\n",
        "\n",
        "for detection in result:\n",
        "    text = detection[1]\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "Zs-FCN04JtTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Primo Esempio, seconda parte, TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_tecnicaTextRecognition/WalmartImmaginiNonStrutturate/5dea7d96-28f6-4710-a696-d4d949cfdf3f.e54036930aeaeebfb494956798c554ee.png'\n",
        "print(\"_2000\")\n",
        "\n",
        "reader = easyocr.Reader(['en']) # Specifica la lingua italiana\n",
        "\n",
        "# Sostituisci 'percorso/della/tua/immagine.jpg' con il percorso effettivo\n",
        "immagine = cv2.imread(data_dir)\n",
        "\n",
        "result = reader.readtext(immagine)\n",
        "\n",
        "for detection in result:\n",
        "    text = detection[1]\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "_qYShz-gKnXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Primo Esempio, seconda parte, TextRecognition, da Immagini sia strutturate Walmart, sia non strutturate Walmart)\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_tecnicaTextRecognition/WalmartImmaginiNonStrutturate/6c9adf3b-8125-48ce-add8-9beec6e84b83.fcfc44f4b9c2f492dd8680e0a9b3c1c0.png'\n",
        "print(\"_2000\")\n",
        "\n",
        "reader = easyocr.Reader(['en']) # Specifica la lingua italiana\n",
        "\n",
        "# Sostituisci 'percorso/della/tua/immagine.jpg' con il percorso effettivo\n",
        "immagine = cv2.imread(data_dir)\n",
        "\n",
        "result = reader.readtext(immagine)\n",
        "\n",
        "for detection in result:\n",
        "    text = detection[1]\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "zC82mHdwLcP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Altri esempi di rotazione immagini generici)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "def skew_image_3d(image_path, output_path, skew_x=0, skew_y=0, skew_z=0):\n",
        "\n",
        "  # Monta Google Drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  print(\"_1000\")\n",
        "\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "  print(\"_2000\")\n",
        "\n",
        "  if img is None:\n",
        "    print(\"_3000\")\n",
        "    raise FileNotFoundError(f\"Immagine non trovata: {image_path}\")\n",
        "\n",
        "  rows, cols = img.shape[:2]\n",
        "\n",
        "  # Matrice di trasformazione: 3x3\n",
        "  skew_matrix = np.float32([\n",
        "    [1, skew_x, skew_z],\n",
        "    [skew_y, 1, skew_z],\n",
        "    [0, 0, 1]\n",
        "  ])\n",
        "\n",
        "  # Trasformazione prospettica\n",
        "  print(\"_4000\")\n",
        "  img_transformed = cv2.warpPerspective(img, skew_matrix, (cols, rows), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "  print(\"_6000\")\n",
        "\n",
        "  cv2.imwrite(output_path, img_transformed)\n",
        "  print(f\"Immagine salvata come: {output_path}\")\n",
        "\n",
        "# Esempio di utilizzo\n",
        "print(\"_100\")\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiDiBase1/pennerummo/Rummo-Penne.jpg'\n",
        "print(\"_200\")\n",
        "\n",
        "data_dir1 = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiTrasformate1/pennerummo/Rummo-Penne_0.03_0_0.jpg'\n",
        "print(\"_300\")\n",
        "\n",
        "skew_image_3d(data_dir, data_dir1, skew_x=0.03, skew_y=0.0, skew_z=0.0)\n",
        "print(\"_400\")\n"
      ],
      "metadata": {
        "id": "GN8aHFrQeE5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qiIUtLj3Fes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRg52_iIoWya",
        "outputId": "ed62a233-c80a-45c6-ea00-78b746874207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Altri esempi rotazione immagini generici)\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiDiBase1/pennerummo/Rummo-Penne.jpg'\n",
        "\n",
        "\n",
        "img = Image.open(data_dir)\n",
        "\n",
        "img_array = np.array(img)\n",
        "\n",
        "angolo = 15  # Angolo di inclinazione in gradi\n",
        "img_inclinata = ndimage.rotate(img_array, angolo, reshape=False)\n",
        "\n",
        "img_inclinata_pil = Image.fromarray(img_inclinata.astype('uint8'))\n",
        "\n",
        "data_dir1 = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiTrasformate1/pennerummo/Rummo-Penne.jpg'\n",
        "\n",
        "img_inclinata_pil.save(data_dir1)\n"
      ],
      "metadata": {
        "id": "viBN5wXHFRD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27cfb151-8cc0-4a2a-e752-2184fa542e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Altri esempi rotazione immagini generici)\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def skew_image_3d(image_path, output_path, skew_x=0, skew_y=0, skew_z=0):\n",
        "\n",
        "  # Apri l'immagine\n",
        "  img = Image.open(image_path)\n",
        "  width, height = img.size\n",
        "  img = img.convert('RGBA')  # Convert to RGBA to handle transparency\n",
        "\n",
        "\n",
        "  # Crea una griglia di coordinate\n",
        "  x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
        "  z = np.zeros_like(x)\n",
        "\n",
        "  # Applica la trasformazione 3D\n",
        "  xyz = np.vstack((x.flatten(), y.flatten(), z.flatten()))\n",
        "\n",
        "  # Matrice di trasformazione 3D\n",
        "  skew_matrix = np.array([\n",
        "    [1, skew_x, skew_z],\n",
        "    [skew_y, 1, skew_z],\n",
        "    [0, 0, 1]\n",
        "  ])\n",
        "\n",
        "  transformed_xyz = np.dot(skew_matrix, xyz)\n",
        "\n",
        "  # Estrai le nuove coordinate\n",
        "  x_new = transformed_xyz[0, :].reshape((height, width)).astype(np.float32)\n",
        "  y_new = transformed_xyz[1, :].reshape((height, width)).astype(np.float32)\n",
        "\n",
        "  # Crea un'immagine vuota per i risultati\n",
        "  img_transformed = Image.new('RGBA', (width, height))\n",
        "\n",
        "  # Copia i pixel dalla vecchia immagine a quella nuova usando le coordinate trasformate\n",
        "  for i in range(height):\n",
        "    for j in range(width):\n",
        "      x_src = int(x_new[i, j])\n",
        "      y_src = int(y_new[i, j])\n",
        "      if 0 <= x_src < width and 0 <= y_src < height:\n",
        "        img_transformed.putpixel((j, i), img.getpixel((x_src, y_src)))\n",
        "\n",
        "  # Salva l'immagine trasformata\n",
        "  img_transformed.save(output_path)\n",
        "  print(f\"Immagine salvata come {output_path}\")\n",
        "\n",
        "# Utilizzo della funzione\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "\n",
        "import pathlib\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiDiBase1/pennerummo/Rummo-Penne.jpg'\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "output_image = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiTrasformate1/pennerummo/Rummo-Penne7.png'\n",
        "\n",
        "# (Todo Personalizzabile)\n",
        "skew_image_3d(data_dir,\n",
        "              output_image,\n",
        "              skew_x=0.1,\n",
        "              skew_y=0.1,\n",
        "              skew_z=0.1)\n",
        "\"\"\"\n",
        "Come funziona\n",
        "Aprire l'immagine: L'immagine viene aperta e convertita in RGBA per gestire la trasparenza.\n",
        "Creare una griglia di coordinate: Una griglia di coordinate 3D viene creata utilizzando numpy.meshgrid.\n",
        "Applicare la trasformazione 3D: Una matrice di trasformazione 3D viene applicata alle coordinate.\n",
        "Trasformare le coordinate: Le nuove coordinate trasformate vengono calcolate e usate per creare un'immagine trasformata.\n",
        "Copiare i pixel: I pixel vengono copiati dalla vecchia immagine a quella nuova usando le coordinate trasformate.\n",
        "Salvare l'immagine: L'immagine trasformata viene salvata nel percorso specificato.\n",
        "Note\n",
        "Il codice sopra potrebbe non gestire perfettamente tutti i casi di trasformazione, soprattutto per valori elevati di inclinazione.\n",
        "Potrebbe essere necessario aggiungere gestione degli errori e miglioramenti per gestire meglio i bordi dell'immagine.\n",
        "Se hai bisogno di una trasformazione 3D più precisa, potresti considerare l'uso di librerie più avanzate come OpenCV.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c812JjFidFaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Altri esempi rotazione immagini generici)\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiDiBase1/pennerummo/Rummo-Penne.jpg'\n",
        "\n",
        "\n",
        "img = Image.open(data_dir)\n",
        "\n",
        "img_array = np.array(img)\n",
        "\n",
        "angolo = 15  # Angolo di inclinazione in gradi\n",
        "img_inclinata = ndimage.rotate(img_array, angolo, reshape=False)\n",
        "\n",
        "img_inclinata_pil = Image.fromarray(img_inclinata.astype('uint8'))\n",
        "\n",
        "data_dir1 = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/prototipo1/setImmaginiTrasformate1/pennerummo/Rummo-Penne.jpg'\n",
        "\n",
        "img_inclinata_pil.save(data_dir1)\n"
      ],
      "metadata": {
        "id": "v9JtGKwxdJOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ifXXvRbP1au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Percorso dell'immagine originale\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/grana_padano_daRidurre/image.png'  # Sostituisci con il percorso effettivo\n",
        "\n",
        "# Apri l'immagine\n",
        "image = Image.open(image_path).convert(\"RGBA\")\n",
        "\n",
        "# Nuova larghezza\n",
        "new_width = 224\n",
        "\n",
        "\n",
        "# Calcola la nuova altezza proporzionalmente\n",
        "ratio = new_width / image.width\n",
        "new_height = int(image.height * ratio)\n",
        "\n",
        "# Crea una nuova immagine con sfondo grigio chiaro\n",
        "# background = Image.new('RGBA', (new_width, new_height), (200, 200, 200, 255))  # Grigio chiaro\n",
        "background = Image.new('RGBA', (new_width, new_height), (230, 230, 230, 255))  # Grigio più chiaro\n",
        "\n",
        "# Incolla l'immagine ridimensionata sullo sfondo\n",
        "resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "background.paste(resized_image, (0, 0), resized_image)\n",
        "\n",
        "# Salva l'immagine ridimensionata con sfondo grigio\n",
        "background.save('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_daRidurre/immagine_ridimensionata_grigio.png')  # Sostituisci con il percorso desiderato"
      ],
      "metadata": {
        "id": "wiiUOqo4U_wc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "311ad7c3-d467-46b6-c3da-f06415ce56f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/grana_padano_daRidurre/image.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1dfa0a47bf73>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Apri l'immagine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Nuova larghezza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/grana_padano_daRidurre/image.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridure proporzionalmente una immagine e riempie la trasparenza con grigio chiaro.\n",
        "!pip install Pillow\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso dell'immagine originale\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/img_ParmigianoReggiano.jpg'  # Sostituisci con il percorso effettivo\n",
        "print(image_path)\n",
        "\n",
        "# Apri l'immagine\n",
        "if os.path.exists(image_path):\n",
        "  image = Image.open(image_path).convert(\"RGBA\")\n",
        "\n",
        "# Nuova larghezza\n",
        "new_width = 180\n",
        "\n",
        "# Nuova altezza\n",
        "new_height = 180\n",
        "\n",
        "# Calcola la nuova altezza proporzionalmente\n",
        "# ratio = new_width / image.width\n",
        "# new_height = int(image.height * ratio)\n",
        "\n",
        "# Crea una nuova immagine con sfondo grigio chiaro\n",
        "# background = Image.new('RGBA', (new_width, new_height), (200, 200, 200, 255))  # Grigio chiaro\n",
        "# background = Image.new('RGBA', (new_width, new_height), (230, 230, 230, 255))  # Grigio più chiaro\n",
        "# background = Image.new('RGBA', (new_width, new_height), (255, 255, 255, 255))\n",
        "\n",
        "# Incolla l'immagine ridimensionata sullo sfondo\n",
        "resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "background.paste(resized_image, (0, 0), resized_image)\n",
        "\n",
        "# Salva l'immagine ridimensionata con sfondo grigio\n",
        "background.save('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/img_ParmigianoReggianoridotto.jpg')  # Sostituisci con il percorso desiderato"
      ],
      "metadata": {
        "id": "lktvFJg-pfvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "928194e5-cbd6-41f5-997c-cae9d4cbe1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/immagine(1).png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3fa7a35a3520>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Incolla l'immagine ridimensionata sullo sfondo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLANCZOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresized_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Per aumentare la nitidezza e la messa a fuoco di un'immagine PNG su Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso dell'immagine su Google Drive\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine.png'  # Sostituisci con il percorso reale\n",
        "\n",
        "# Funzione per aumentare la nitidezza e la messa a fuoco\n",
        "def sharpen_image(image_path):\n",
        "  \"\"\"\n",
        "  Aumenta la nitidezza e la messa a fuoco di un'immagine.\n",
        "\n",
        "  Args:\n",
        "    image_path: Il percorso dell'immagine.\n",
        "\n",
        "  Returns:\n",
        "    L'immagine nitida come un array NumPy.\n",
        "  \"\"\"\n",
        "  # Carica l'immagine con Pillow\n",
        "  image_pil = Image.open(image_path).convert(\"RGB\")  # Converte in RGB se necessario\n",
        "\n",
        "  # Converti l'immagine PIL in un array NumPy\n",
        "  image_np = img_to_array(image_pil)\n",
        "\n",
        "  image_np = cv2.GaussianBlur(image_np, (0, 0), 3)  # Applica un leggero blur\n",
        "  sharpened_image_np = cv2.addWeighted(image_np, 1.5, image_np, -0.5, 0)\n",
        "\n",
        "\n",
        "  # Applica un filtro di nitidezza\n",
        "  image_pil = Image.fromarray(image_np.astype('uint8'))\n",
        "  # Applica il filtro di nitidezza più volte\n",
        "  for _ in range(3):  # Applica il filtro 3 volte\n",
        "    image_pil = image_pil.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "  # image_pil = image_pil.filter(ImageFilter.SHARPEN)\n",
        "  # Puoi anche sperimentare con altri filtri come:\n",
        "  # image_pil = image_pil.filter(ImageFilter.DETAIL)\n",
        "  # image_pil = image_pil.filter(ImageFilter.EDGE_ENHANCE)\n",
        "\n",
        "  # Riconverti l'immagine in un array NumPy\n",
        "  image_np = np.array(image_pil)\n",
        "\n",
        "  return image_np\n",
        "\n",
        "# Aumenta la nitidezza dell'immagine\n",
        "sharpened_image = sharpen_image(image_path)\n",
        "\n",
        "# Converti l'immagine nitida in un'immagine PIL\n",
        "sharpened_image_pil = array_to_img(sharpened_image)\n",
        "\n",
        "# Salva l'immagine nitida\n",
        "output_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine_nitida.png'  # Sostituisci con il percorso desiderato\n",
        "sharpened_image_pil.save(output_path)\n",
        "\n",
        "print(f\"Immagine nitida salvata in: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VNuHDxAQQ_z",
        "outputId": "aaddd8ea-e803-4db9-b328-869178c23deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Immagine nitida salvata in: /content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine_nitida.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import cv2  # Importa OpenCV\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso dell'immagine su Google Drive\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine.png'  # Sostituisci con il percorso reale\n",
        "\n",
        "# Funzione per aumentare la nitidezza e la messa a fuoco\n",
        "def sharpen_image(image_path, method=\"pillow\", intensity=1, deblur=False, contrast=False, high_pass=False):\n",
        "  \"\"\"\n",
        "  Aumenta la nitidezza e la messa a fuoco di un'immagine.\n",
        "\n",
        "  Args:\n",
        "    image_path: Il percorso dell'immagine.\n",
        "    method: Il metodo di nitidezza da utilizzare (\"pillow\", \"kernel\", \"opencv\").\n",
        "    intensity: L'intensità della nitidezza (1 per default).\n",
        "    deblur: Se applicare il deblurring (False per default).\n",
        "    contrast: Se aumentare il contrasto (False per default).\n",
        "\n",
        "  Returns:\n",
        "    L'immagine nitida come un array NumPy.\n",
        "  \"\"\"\n",
        "  # Carica l'immagine con Pillow\n",
        "  image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # Converti l'immagine PIL in un array NumPy\n",
        "  image_np = img_to_array(image_pil)\n",
        "\n",
        "  if deblur:\n",
        "    # Applica deblurring con cv2.deblur_motion\n",
        "    kernel_size = (5, 5)\n",
        "    kernel_motion_blur = np.zeros(kernel_size)\n",
        "    kernel_motion_blur[int(kernel_size[0]/2), :] = np.ones(kernel_size[1])\n",
        "    kernel_motion_blur = kernel_motion_blur / kernel_size[1]\n",
        "    image_np = cv2.filter2D(image_np, -1, kernel_motion_blur)\n",
        "\n",
        "  if contrast:\n",
        "    # Aumenta il contrasto con cv2.equalizeHist\n",
        "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
        "    image_np = cv2.equalizeHist(image_np)\n",
        "    image_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "  if high_pass:\n",
        "    # Define a high-pass filter kernel\n",
        "    kernel = np.array([[-1, -1, -1],\n",
        "                      [-1,  9, -1],\n",
        "                      [-1, -1, -1]])\n",
        "\n",
        "    # Apply the high-pass filter\n",
        "    filtered_image = cv2.filter2D(image_np, -1, kernel)\n",
        "\n",
        "  if method == \"pillow\":\n",
        "    # Applica il filtro di nitidezza di Pillow più volte\n",
        "    image_pil = Image.fromarray(image_np.astype('uint8'))\n",
        "    for _ in range(intensity):\n",
        "        image_pil = image_pil.filter(ImageFilter.SHARPEN)\n",
        "    image_np = np.array(image_pil)  # Riconverti in NumPy array\n",
        "  elif method == \"kernel\":\n",
        "    # Definisci un kernel di nitidezza personalizzato\n",
        "    sharpen_kernel = ImageFilter.Kernel(\n",
        "        (3, 3),\n",
        "        (-1, -1, -1,\n",
        "         -1,  9, -1,\n",
        "         -1, -1, -1),\n",
        "        1, 0)\n",
        "    # Applica il kernel personalizzato\n",
        "    image_pil = image_pil.filter(sharpen_kernel)\n",
        "    image_np = np.array(image_pil)  # Riconverti in NumPy array\n",
        "  elif method == \"opencv\":\n",
        "    # Applica Unsharp Masking con OpenCV\n",
        "    image_np = cv2.GaussianBlur(image_np, (0, 0), 3)\n",
        "    sharpened_image_np = cv2.addWeighted(image_np, 1.5 + (intensity - 1) * 0.5, image_np, -0.5 - (intensity - 1) * 0.5, 0)\n",
        "    image_np = sharpened_image_np\n",
        "\n",
        "  return image_np\n",
        "\n",
        "\n",
        "# Aumenta la nitidezza dell'immagine\n",
        "# Scegli il metodo e l'intensità desiderati\n",
        "sharpened_image = sharpen_image(image_path, method=\"opencv\", intensity=3) # method=\"pillow\", \"kernel\", \"opencv\"\n",
        "\n",
        "# Converti l'immagine nitida in un'immagine PIL\n",
        "sharpened_image_pil = array_to_img(sharpened_image)\n",
        "\n",
        "# Salva l'immagine nitida\n",
        "output_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine_nitida.png'  # Sostituisci con il percorso desiderato\n",
        "sharpened_image_pil.save(output_path)\n",
        "\n",
        "print(f\"Immagine nitida salvata in: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4kunYZ_S6Tt",
        "outputId": "f4f50caa-6b5e-4097-843c-bcf8f130cd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Immagine nitida salvata in: /content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_base_4_1/immagine_nitida.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc8mvY2ZzG0m",
        "outputId": "e9bd169d-8bb7-4aac-936a-9cf974d20908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Eseguire in parallelo) Un esempio di tecniche di data augmentation offerte da Keras.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from scipy import datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "\n",
        "# Monta Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisci il percorso delle tue immagini:\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_12112024/daRidimensionare/pecorinoRomano_0'\n",
        "\n",
        "\n",
        "print (\"imgTrasformation_100\")\n",
        "\n",
        "# Carica le immagini:\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "images = []\n",
        "# (parametrizzabile)\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "for filename in os.listdir(data_dir):\n",
        "       if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Assicurati di caricare solo immagini\n",
        "           img_path = os.path.join(data_dir, filename)\n",
        "           img = image.load_img(img_path, target_size=(img_height, img_width))  # Utilizza le stesse dimensioni definite prima\n",
        "           img_array = image.img_to_array(img)\n",
        "           images.append(img_array)\n",
        "\n",
        "print(\"imgTrasformation_200\")\n",
        "\n",
        "# Applica data augmentation:\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    brightness_range: Variare la luminosità delle immagini può aiutare il modello a generalizzare meglio a diverse condizioni di illuminazione.\n",
        "    channel_shift_range: Modificare leggermente i canali di colore può aiutare il modello a non fare troppo affidamento su specifici colori per la classificazione.\n",
        "    rotation_range, width_shift_range, height_shift_range, shear_range, zoom_range: Queste trasformazioni geometriche aiutano il modello a imparare a riconoscere gli oggetti indipendentemente dalla loro posizione, dimensione o orientamento nell'immagine.\n",
        "    horizontal_flip: Capovolgere orizzontalmente le immagini è particolarmente utile se gli oggetti nelle tue immagini possono apparire in entrambe le direzioni (ad esempio, persone, animali, veicoli).\n",
        "    fill_mode: Il modo in cui vengono riempite le aree vuote create dalle trasformazioni geometriche può influenzare la qualità delle immagini aumentate. 'nearest' è una scelta comune, ma potresti sperimentare anche con altri metodi come 'reflect' o 'wrap'.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# (Parametrizzabili)\n",
        "datagen = ImageDataGenerator(\n",
        "       brightness_range=[0.8, 1.2],  # Varia la luminosità.\n",
        "       channel_shift_range=0.2,      # Varia i canali di colore.\n",
        "       rotation_range=6,             # (0.0)(0.2) Ruota le immagini casualmente.\n",
        "       width_shift_range=0.0,        # trasla le immagini orizzontalmente.\n",
        "       height_shift_range=0.0,       # Trasla le immagini verticalmente.\n",
        "       shear_range=0.0,              # Applica una trasformazione di shear (inclinazione).\n",
        "       zoom_range=0.0,\n",
        "       horizontal_flip=False,        # True: Riflette le immagini orizzontalmente con una probabilità del 50%.\n",
        "       # vertical_flip=False,        # True: Capovolge casualmente le immagini verticalmente.\n",
        "       # fill_mode='nearest'         # (No Strisce) ('nearest': Definisce come riempire i pixel vuoti che possono essere creati dalle trasformazioni (in questo caso, usa il valore del pixel più vicino).)\n",
        "       fill_mode='wrap'              # oppure 'reflect': Provare per eliminare le strisce di colore.\n",
        ")\n",
        "\n",
        "augmented_images = []\n",
        "for img in images:\n",
        "       img = img.reshape((1,) + img.shape)  # Aggiungi una dimensione per il batch\n",
        "       i = 0\n",
        "       for batch in datagen.flow(img, batch_size=1):\n",
        "           augmented_images.append(batch[0])\n",
        "           i += 1\n",
        "           if i >= 15:  # Genera n immagini aumentate per ogni immagine originale\n",
        "               break\n",
        "\n",
        "# Salva le immagini aumentate:\n",
        "\n",
        "print(\"imgTrasformation_300\")\n",
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_12112024/daRidimensionare/pecorinoRomano_augmentation_0'  # Definisci il percorso in cui salvare le immagini\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for i, img in enumerate(augmented_images):\n",
        "       filename = os.path.join(output_dir, f'pecorinoRomano_{i}.jpg')\n",
        "       image.save_img(filename, img)\n",
        "\n",
        "print(\"imgTrasformation_600\")"
      ],
      "metadata": {
        "id": "FGx4mSMXxw8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22efb8b0-5c3a-4183-b7fe-ae1585b043e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "imgTrasformation_100\n",
            "imgTrasformation_200\n",
            "imgTrasformation_300\n",
            "imgTrasformation_600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ICCx5PnFHm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# pip install Pillow numpy\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ma1RXnxhFQN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.9.0 # Installa TensorFlow 2.9.0\n",
        "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime # Installa il runtime di TensorFlow Lite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF5_V4926OaM",
        "outputId": "a1dd77a9-e48e-44e5-9582-e6ddcab3c285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.9.0 in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://google-coral.github.io/py-repo/\n",
            "Requirement already satisfied: tflite_runtime in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite_runtime) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prova del modello tFLite quantizzato\n",
        "\"\"\"\n",
        "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n",
        "import tflite_runtime.interpreter as tflite\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Carica il modello\n",
        "interpreter = tflite.Interpreter(model_path=\"model_quantizzato.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni dettagli input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Carica l'immagine di test\n",
        "# img = Image.open(\"/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024/test_logo_dop_0/ctcb_it.jpg\").resize((img_width, img_height)) # Sostituisci con il percorso della tua immagine\n",
        "# img = np.array(img).astype(np.float32) / 255.0 # Normalizza l'immagine\n",
        "\n",
        "# Carica l'immagine di test PNG\n",
        "img = Image.open(\"/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024/test_logo_dop_0/ctcb_it.png\").convert('RGB')  # Converti in RGB se necessario\n",
        "img = img.resize((278, 278))\n",
        "# Converti l'immagine in un array NumPy e normalizza\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = img.astype(np.float32)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "# img = img.astype(np.float32)\n",
        "# img = np.array(img).astype(np.float32) / 255.0 # Normalizza l'immagine\n",
        "# img = img / 255.0\n",
        "\n",
        "# Imposta l'input del modello\n",
        "input_details = interpreter.get_input_details()\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\n",
        "# Ottieni l'output del modello\n",
        "output_details = interpreter.get_output_details()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "# Imposta il tensore di input\n",
        "input_data = np.expand_dims(img, axis=0)\n",
        "input_details = interpreter.get_input_details()\n",
        "print(input_details)\n",
        "print(\"1000\")\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['dtype'], input_data)\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\n",
        "# Ottieni i risultati\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "\n",
        "# Trova la classe predetta\n",
        "predicted_class = np.argmax(output_data)\n",
        "\n",
        "print(\"Classe predetta:\", predicted_class)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RN6yLPoC9Ldl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Prova di un modello keras)\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1. Carica il modello Keras\n",
        "model = keras.models.load_model('my_saved_model.keras')\n",
        "\n",
        "# 2. Prepara l'immagine JPG\n",
        "img_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024/img_ParmigianoReggiano.jpg'\n",
        "img = Image.open(img_path).resize((180, 180))  # Ridimensiona a 224x224\n",
        "\n",
        "# Converti l'immagine in un array NumPy\n",
        "img_array = np.array(img)\n",
        "\n",
        "# Pre-elabora l'immagine (se necessario)\n",
        "img_array = img_array / 255.0  # Normalizza i valori dei pixel tra 0 e 1\n",
        "img_array = img_array.astype(np.float32)  # Converti il tipo di dati in float32\n",
        "\n",
        "# Aggiungi una dimensione batch se il modello lo richiede\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# 3. Esegui la previsione\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# 4. Stampa o analizza le previsioni\n",
        "print(predictions)\n",
        "print(\"1000\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIOEBluJS5Xq",
        "outputId": "a2640dd5-3238-494f-e589-46a26fc2c5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step\n",
            "[[0.1250174  0.12498847 0.12500793 0.12496062 0.12501617 0.1249769\n",
            "  0.12502506 0.12500747]]\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Prova di un modello keras)\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1. Carica il modello Keras\n",
        "model = keras.models.load_model('my_saved_model.keras')\n",
        "\n",
        "# 2. Prepara l'immagine JPG\n",
        "img_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024/img_ParmigianoReggiano.jpg'\n",
        "img = Image.open(img_path).resize((180, 180))  # Ridimensiona a 224x224\n",
        "\n",
        "# Converti l'immagine in un array NumPy\n",
        "img_array = np.array(img)\n",
        "\n",
        "# Pre-elabora l'immagine (se necessario)\n",
        "img_array = img_array / 255.0  # Normalizza i valori dei pixel tra 0 e 1\n",
        "img_array = img_array.astype(np.float32)  # Converti il tipo di dati in float32\n",
        "\n",
        "# Aggiungi una dimensione batch se il modello lo richiede\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# 3. Esegui la previsione\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# 4. Stampa o analizza le previsioni\n",
        "print(predictions)\n",
        "print(\"1000\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2640dd5-3238-494f-e589-46a26fc2c5fd",
        "id": "n1zFhCEiiSv3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step\n",
            "[[0.1250174  0.12498847 0.12500793 0.12496062 0.12501617 0.1249769\n",
            "  0.12502506 0.12500747]]\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Prova di un modello keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1. Carica il modello Keras\n",
        "model = keras.models.load_model('my_saved_model_2.keras')\n",
        "\n",
        "# ... (import other necessary libraries)\n",
        "\n",
        "# Assuming you have loaded your model into the 'model' variable\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  \"\"\"Loads, resizes, and converts an image to RGB for prediction.\"\"\"\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)  # Or decode_png, etc.\n",
        "  image = tf.image.resize(image, [180, 180])  # Resize to your model's input shape\n",
        "  # image = tf.image.rgb_to_grayscale(image) # not necessary if channels already 3 with decode_jpeg(..., channels=3)\n",
        "  # Normalize if necessary (e.g., image = image / 255.0)\n",
        "  return image\n",
        "\n",
        "# Path to the image you want to test\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_daRidurre/immagine_ridimensionata_grigio.png' # replace with your image path\n",
        "\n",
        "# Preprocess the image\n",
        "input_image = preprocess_image(image_path)\n",
        "\n",
        "# Add a batch dimension if necessary\n",
        "input_image = np.expand_dims(input_image, axis=0)\n",
        "\n",
        "# Make the prediction\n",
        "predictions = model.predict(input_image)\n",
        "\n",
        "# Process the predictions (e.g., get the class with the highest probability)\n",
        "# ...\n",
        "\n",
        "# 4. Stampa o analizza le previsioni\n",
        "predicted_class_index = np.argmax(predictions[0])\n",
        "print(predictions)\n",
        "print(\"1000\")\n",
        "print(predicted_class_index)\n",
        "print(\"2000\")"
      ],
      "metadata": {
        "id": "I428bBdAiT3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Prova di un modello keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1. Carica il modello Keras\n",
        "model = keras.models.load_model('model1.keras')\n",
        "\n",
        "# ... (import other necessary libraries)\n",
        "\n",
        "# Assuming you have loaded your model into the 'model' variable\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  \"\"\"Loads, resizes, and converts an image to RGB for prediction.\"\"\"\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)  # Or decode_png, etc.\n",
        "  image = tf.image.resize(image, [180, 180])  # Resize to your model's input shape\n",
        "  # image = tf.image.rgb_to_grayscale(image)  # not necessary if channels already 3 with decode_jpeg(..., channels=3)\n",
        "  # Normalize if necessary (e.g., image = image / 255.0)\n",
        "  return image\n",
        "\n",
        "# Path to the image you want to test\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/parmigiano_reggiano_daRidurre/immagine_ridimensionata_grigio.png' # replace with your image path\n",
        "\n",
        "# Preprocess the image\n",
        "input_image = preprocess_image(image_path)\n",
        "\n",
        "# Add a batch dimension if necessary\n",
        "input_image = np.expand_dims(input_image, axis=0)\n",
        "\n",
        "# Make the prediction\n",
        "predictions = model.predict(input_image)\n",
        "\n",
        "# Process the predictions (e.g., get the class with the highest probability)\n",
        "# ...\n",
        "\n",
        "# 4. Stampa o analizza le previsioni\n",
        "predicted_class_index = np.argmax(predictions[0])\n",
        "print(predictions)\n",
        "print(\"1000\")\n",
        "print(predicted_class_index)\n",
        "print(\"2000\")"
      ],
      "metadata": {
        "id": "lFNbLwLZ0R8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Prova di un modello keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# 1. Carica il modello Keras\n",
        "model = keras.models.load_model('my_saved_model_2.keras')\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Rescale pixel values\n",
        "    shear_range=0.2,  # Apply shear transformations\n",
        "    zoom_range=0.2,   # Apply zoom transformations\n",
        "    horizontal_flip=True  # Flip images horizontally\n",
        ")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/parmigiano_reggiano_4',  # Replace with the path to your training data\n",
        "    target_size=(224, 224),  # Set the desired image size\n",
        "    batch_size=32,  # Specify the batch size\n",
        "    class_mode='categorical'  # Set the class mode (e.g., 'categorical', 'binary')\n",
        ")\n",
        "model.fit(train_generator,\n",
        "  epochs=10,\n",
        "  steps_per_epoch=100,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=50)\n",
        "\n",
        "\n",
        "# ... (your model and preprocess_image function)\n",
        "\n",
        "# @tf.function\n",
        "def preprocess_image(image_path):\n",
        "  \"\"\"Loads, resizes, and converts an image to RGB for prediction.\"\"\"\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.image.resize(image, [224, 224])  # Resize to 224x224\n",
        "  return image\n",
        "\n",
        "# Load, preprocess, and add batch dimension\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/_000_000_loghiTest/parmigiano_reggiano_daRidurre/immagine_ridimensionata_grigio.png'\n",
        "input_image = preprocess_image(image_path)\n",
        "input_image = tf.expand_dims(input_image, axis=0)\n",
        "input_image = tf.cast(input_image, tf.float32)\n",
        "\n",
        "# Make the prediction\n",
        "predictions = predict_image(input_image) # using predict_image function wrapped with tf.function\n",
        "\n",
        "# ... (rest of your prediction processing)\n",
        "\n",
        "# Process the predictions (e.g., get the class with the highest probability)\n",
        "# ...\n",
        "\n",
        "# 4. Stampa o analizza le previsioni\n",
        "predicted_class_index = np.argmax(predictions[0])\n",
        "print(predictions)\n",
        "print(\"1000\")\n",
        "print(predicted_class_index)\n",
        "print(\"2000\")\n",
        "# To get the label for index 2, for example:\n",
        "label = list(class_indices.keys())[list(class_indices.values()).index(6)]\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "b6181fde-ea60-40b1-c7a1-ed0724cc77b9",
        "id": "c_8dzzwA0TxG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/parmigiano_reggiano_4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d7bd469efe0>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mhorizontal_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Flip images horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;34m'/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/parmigiano_reggiano_4'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replace with the path to your training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Set the desired image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m     \"\"\"\n\u001b[0;32m-> 1469\u001b[0;31m     return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1470\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m       \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m           \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_11112024/parmigiano_reggiano_4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tflite-model-maker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-LAL5tmCS4R",
        "outputId": "15fe2c09-77d5-4cfd-b8c0-77f88d0a6735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tflite-model-maker as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione modello da Model Maker\n",
        "!pip uninstall -y tflite-model-maker\n",
        "!pip cache purge\n",
        "!python3 -m venv my_env\n",
        "!source my_env/bin/activ\n",
        "!pip install 'tflite-model-maker==0.4.3' 'numba==0.56.4' 'scann==1.2.6' 'tflite-support==0.4.4'\n",
        "# !pip install tflite-model-maker==0.4.3 scann==1.2.5 numba==0.56.4 tflite-support==0.4.3\n",
        "!pip install --no-dependencies --force-reinstall tflite-model-maker==0.4.3\n",
        "!pip install numba==0.56.4 scann==1.2.6 tflite-support==0.4.4\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "# from tflite_model_maker import model_spec\n",
        "# from tflite_model_maker.image_classifier import EfficientNetLite0Spec\n",
        "\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.config import Export\n",
        "\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.image_classifier import DataLoader\n",
        "from tflite_model_maker.model_spec import efficientnet_lite0 # or other model spec you need\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024'\n"
      ],
      "metadata": {
        "id": "Rb_ONT8D9n1c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.10.0\n",
        "# !pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset directory\n",
        "dataset_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_11112024'\n",
        "\n",
        "# Image dimensions\n",
        "img_width, img_height = 150, 150  # Adjust as needed\n",
        "\n",
        "# Create an ImageDataGenerator for loading and preprocessing images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Set aside 20% for validation\n",
        ")\n",
        "\n",
        "# Create training and validation data generators (modified)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    # Specify the image file format\n",
        "    classes=['grana_padano_3', 'logo_dop_0', 'parmigiano_reggiano_3','parmigiano_reggiano_4','pecorini_romano_0','pecorini_romano_5','pecorini_romano_6','pecorini_romano_8'], # replace with actual label names\n",
        "    save_format='jpg'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    # Specify the image file format\n",
        "    classes=['grana_padano_3', 'logo_dop_0', 'parmigiano_reggiano_3','parmigiano_reggiano_4','pecorini_romano_0','pecorini_romano_5','pecorini_romano_6','pecorini_romano_8'], # replace with actual label names\n",
        "    save_format='jpg'\n",
        ")\n",
        "\n",
        "# Create a simple CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 classes for your subfolders\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_generator, epochs=10, validation_data=validation_generator) # Adjust epochs as needed\n",
        "\n",
        "# Save the model\n",
        "model.save('my_image_classifier.h5')"
      ],
      "metadata": {
        "id": "btD1z7lTJkn5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b696577-7e33-4359-b18e-caa9ecc61c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.68.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 100 images belonging to 8 classes.\n",
            "Found 25 images belonging to 8 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-2-e4ea6eb8a621>\", line 68, in <cell line: 68>\n      model.fit(train_generator, epochs=10, validation_data=validation_generator) # Adjust epochs as needed\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,3] labels_size=[32,8]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_1845]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e4ea6eb8a621>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adjust epochs as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-2-e4ea6eb8a621>\", line 68, in <cell line: 68>\n      model.fit(train_generator, epochs=10, validation_data=validation_generator) # Adjust epochs as needed\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,3] labels_size=[32,8]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_1845]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0 # Installa TensorFlow 2.12.0 per compatibilità\n",
        "!pip install tf-nightly\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tf-nightly"
      ],
      "metadata": {
        "id": "t-XChKfaeFWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d2d0ad-eb86-41b8-dbe0-00df2e40efad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.68.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.0)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Collecting tf-nightly\n",
            "  Using cached tf_nightly-2.19.0.dev20241121-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.68.0)\n",
            "Collecting tb-nightly~=2.19.0.a (from tf-nightly)\n",
            "  Using cached tb_nightly-2.19.0a20241121-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras-nightly>=3.6.0.dev (from tf-nightly)\n",
            "  Using cached keras_nightly-3.7.0.dev2024112703-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tf-nightly)\n",
            "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.19.0.a->tf-nightly) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.6.0.dev->tf-nightly) (0.1.2)\n",
            "Using cached tf_nightly-2.19.0.dev20241121-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (631.2 MB)\n",
            "Using cached keras_nightly-3.7.0.dev2024112703-py3-none-any.whl (1.2 MB)\n",
            "Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Using cached tb_nightly-2.19.0a20241121-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: numpy, tb-nightly, keras-nightly, tf-nightly\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tflite-model-maker 0.4.3 requires fire>=0.3.1, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires neural-structured-learning>=1.3.1, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires scann==1.2.6, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-addons>=0.11.2, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-model-optimization>=0.5, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflowjs<3.19.0,>=2.4.0, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tf-models-official==2.3.0, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tflite-support>=0.4.2, which is not installed.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "pytensor 2.26.3 requires numpy<2,>=1.17.0, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires librosa==0.8.1, but you have librosa 0.10.2.post1 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires matplotlib<3.5.0,>=3.0.3, but you have matplotlib 3.8.0 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires numpy<1.23.4,>=1.17.3, but you have numpy 2.1.3 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-hub<0.13,>=0.7.0; python_version >= \"3\", but you have tensorflow-hub 0.16.1 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 2.2.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-nightly-3.7.0.dev2024112703 numpy-2.1.3 tb-nightly-2.19.0a20241121 tf-nightly-2.19.0.dev20241121\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m596.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tflite-model-maker 0.4.3 requires fire>=0.3.1, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires neural-structured-learning>=1.3.1, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires scann==1.2.6, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-addons>=0.11.2, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-model-optimization>=0.5, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tensorflowjs<3.19.0,>=2.4.0, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tf-models-official==2.3.0, which is not installed.\n",
            "tflite-model-maker 0.4.3 requires tflite-support>=0.4.2, which is not installed.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.26.3 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires librosa==0.8.1, but you have librosa 0.10.2.post1 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires matplotlib<3.5.0,>=3.0.3, but you have matplotlib 3.8.0 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires numpy<1.23.4,>=1.17.3, but you have numpy 2.0.2 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires tensorflow-hub<0.13,>=0.7.0; python_version >= \"3\", but you have tensorflow-hub 0.16.1 which is incompatible.\n",
            "tflite-model-maker 0.4.3 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 2.2.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.7.0 numpy-2.0.2 tensorboard-2.18.0 tensorflow-2.18.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.10/dist-packages (2.19.0.dev20241121)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.68.0)\n",
            "Requirement already satisfied: tb-nightly~=2.19.0.a in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.19.0a20241121)\n",
            "Requirement already satisfied: keras-nightly>=3.6.0.dev in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.7.0.dev2024112703)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.19.0.a->tf-nightly) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.6.0.dev->tf-nightly) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Monta Google Drive (se necessario)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Definisci il percorso del tuo dataset\n",
        "dataset_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_12112024'  # Sostituisci con il tuo percorso\n",
        "\n",
        "# 2. Funzione per caricare e convertire le immagini\n",
        "def load_and_convert_image(image_path, target_size=(180, 180)):  # Aggiungi target_size\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize(target_size)  # Ridimensiona l'immagine\n",
        "        image_np = np.array(image)\n",
        "        return image_np\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'apertura dell'immagine {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# 3. Carica le immagini e crea le etichette\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "class_names = []\n",
        "\n",
        "for class_folder in os.listdir(dataset_path):\n",
        "    class_path = os.path.join(dataset_path, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        class_names.append(class_folder)  # Aggiungi il nome della classe\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_file_path = os.path.join(class_path, image_file)\n",
        "            if image_file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                image_np = load_and_convert_image(image_file_path)  # Apri l'immagine qui\n",
        "                if image_np is not None:  # Controlla se l'immagine è stata caricata correttamente\n",
        "                    images.append(image_np)\n",
        "                    labels.append(class_names.index(class_folder))\n",
        "\n",
        "# 4. Converti le liste in array NumPy\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Livello100\")\n",
        "print(images[0].shape)  # Stampa la forma della prima immagine\n",
        "\n",
        "# 5. Pre-elabora le immagini (ridimensionamento, normalizzazione)\n",
        "images = tf.image.resize(images, [180, 180])  # Ridimensiona a 180x180\n",
        "images = images / 255.0  # Normalizza i valori dei pixel tra 0 e 1\n",
        "\n",
        "# 6. Crea il modello\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(len(class_names), activation='softmax')  # Usa il numero di classi\n",
        "])\n",
        "\n",
        "# 7. Compila il modello\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 8. Addestra il modello\n",
        "model.fit(images, labels, epochs=10)\n",
        "\n",
        "# 9. Salva il modello (opzionale)\n",
        "model.save('logo_classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hqf1pZ4zFYOw",
        "outputId": "fe895680-d6bf-4d18-f5f0-02fd58e445b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b9da5c0fd239>\", line 1, in <cell line: 1>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 2, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 8, in <module>\n",
            "    from keras.api import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n",
            "    from keras.src.activations import deserialize\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\", line 13, in <module>\n",
            "    from keras.src import visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/__init__.py\", line 2, in <module>\n",
            "    from keras.src.visualization import plot_image_gallery\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/plot_image_gallery.py\", line 13, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 161, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 57, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 143, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b9da5c0fd239>\", line 1, in <cell line: 1>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 2, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
            "    from keras.api import visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
            "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
            "    from matplotlib import patches  # For legend patches\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 161, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 57, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 143, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9da5c0fd239>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 3. Carica le immagini e crea le etichette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Monta Google Drive (se necessario)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Definisci il percorso del tuo dataset\n",
        "dataset_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_12112024'  # Sostituisci con il tuo percorso\n",
        "\n",
        "# 2. Funzione per caricare e convertire le immagini\n",
        "def load_and_convert_image(image_path, target_size=(180, 180)):  # Aggiungi target_size\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize(target_size)  # Ridimensiona l'immagine\n",
        "        image_np = np.array(image)\n",
        "        return image_np\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'apertura dell'immagine {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# 3. Carica le immagini e crea le etichette\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "class_names = []\n",
        "\n",
        "for class_folder in os.listdir(dataset_path):\n",
        "    class_path = os.path.join(dataset_path, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        class_names.append(class_folder)  # Aggiungi il nome della classe\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_file_path = os.path.join(class_path, image_file)\n",
        "            if image_file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                image_np = load_and_convert_image(image_file_path)  # Apri l'immagine qui\n",
        "                if image_np is not None:  # Controlla se l'immagine è stata caricata correttamente\n",
        "                    images.append(image_np)\n",
        "                    labels.append(class_names.index(class_folder))\n",
        "\n",
        "# 4. Converti le liste in array NumPy\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Livello100\")\n",
        "print(images[0].shape)  # Stampa la forma della prima immagine\n",
        "\n",
        "# 5. Pre-elabora le immagini (ridimensionamento, normalizzazione)\n",
        "images = tf.image.resize(images, [180, 180])  # Ridimensiona a 180x180\n",
        "images = images / 255.0  # Normalizza i valori dei pixel tra 0 e 1\n",
        "\n",
        "# 6. Crea il modello\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(len(class_names), activation='softmax')  # Usa il numero di classi\n",
        "])\n",
        "\n",
        "# 7. Compila il modello\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 8. Addestra il modello\n",
        "model.fit(images, labels, epochs=10)\n",
        "\n",
        "# 9. Salva il modello (opzionale)\n",
        "model.save('logo_classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "cbce15a6-b3ea-4152-e961-4245769d3193",
        "id": "JhHX0WcALFN7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b9da5c0fd239>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 3. Carica le immagini e crea le etichette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (27112024 Creazione modello TensorFlow)\n",
        "\n",
        "# !pip install --upgrade numpy\n",
        "# !pip uninstall numpy\n",
        "# !pip uninstall tensorflow\n",
        "# !pip install numpy\n",
        "# !pip install --upgrade numpy\n",
        "# !pip install tensorflow\n",
        "\n",
        "!pip uninstall numpy\n",
        "!pip uninstall tensorflow\n",
        "!pip install numpy\n",
        "!pip install tensorflow\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso del dataset su Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_12112024'  # Sostituisci con il percorso del tuo dataset\n",
        "\n",
        "# Funzione per caricare e convertire le immagini\n",
        "def load_and_convert_image(image_path, target_size=(180, 180)):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')  # Converti in RGB\n",
        "        image_np = np.array(image)  # Converti in array NumPy PRIMA del resize\n",
        "        image_np = tf.image.resize(image_np, target_size)  # Ridimensiona con tf.image.resize\n",
        "        return image_np.numpy()  # Converti il tensore in array NumPy\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'apertura dell'immagine {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Carica le immagini e crea le etichette\n",
        "images = []  # Inizializza images come lista\n",
        "labels = []\n",
        "class_names = []\n",
        "\n",
        "for class_folder in os.listdir(dataset_path):\n",
        "    class_path = os.path.join(dataset_path, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        class_names.append(class_folder)\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_file_path = os.path.join(class_path, image_file)\n",
        "            if image_file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                image_np = load_and_convert_image(image_file_path)\n",
        "                if image_np is not None:\n",
        "                    images.append(image_np)  # Aggiungi l'immagine alla lista\n",
        "                    labels.append(class_names.index(class_folder))\n",
        "\n",
        "# Converti la lista in un array NumPy (solo una volta)\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Pre-elabora le immagini (normalizzazione)\n",
        "images = images / 255.0\n",
        "\n",
        "# Crea il modello\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compila il modello\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Addestra il modello\n",
        "model.fit(images, labels, epochs=10)\n",
        "\n",
        "# Salva il modello\n",
        "model.save('image_classifier.h5')\n",
        "# Salva class_names\n",
        "with open('class_names.txt', 'w') as f:\n",
        "    for class_name in class_names:\n",
        "        f.write(class_name + '\\n')\n",
        "\n",
        "print(\"Modello e nomi delle classi salvati!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b511052-12b5-4584-9571-cec774721142",
        "collapsed": true,
        "id": "ScacJZMFLHI8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-2.0.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.18.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires tensorflow>=2.2.0, which is not installed.\n",
            "tf-models-official 2.3.0 requires tensorflow>=2.3.0, which is not installed.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "pytensor 2.26.3 requires numpy<2,>=1.17.0, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow-model-optimization 0.8.0 requires numpy~=1.23, but you have numpy 2.1.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.3\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Installing collected packages: numpy, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.26.3 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow-model-optimization 0.8.0 requires numpy~=1.23, but you have numpy 2.0.2 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2 tensorflow-2.18.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.4100 - loss: 2.3107\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9232 - loss: 0.3021\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0124\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0032\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.5189e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.1703e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4341e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 6.3945e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.9916e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.1848e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello e nomi delle classi salvati!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (27112024 Prova modello TensorFlow)\n",
        "!pip install --upgrade numpy\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Carica il modello\n",
        "model = keras.models.load_model('image_classifier.h5')\n",
        "\n",
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carica e pre-elabora una nuova immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/pecorino_romano_0_1.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/pecorino_romano_bold_8_1.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/ctcb_it.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/granapadano_kleecks_cdn_com.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/Logo-pecorino-dop-100.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/parmigiano_reggiano_x_12.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/3_logo.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/marchio_consorzio.png'  # Sostituisci con il percorso della tua immagine\n",
        "# (ERRORE sbaglia classe) image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/sup_marchio_consorzio.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/marchio_dop.png'  # Sostituisci con il percorso della tua immagine\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/images_2019_5_PARMIGIANO_REGGIANO.png'  # Sostituisci con il percorso della tua immagine\n",
        "\n",
        "\n",
        "\n",
        "# Carica class_names dal file\n",
        "with open('class_names.txt', 'r') as f:\n",
        "    class_names = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = image.resize((180, 180))\n",
        "image_np = np.array(image)\n",
        "image_np = image_np / 255.0\n",
        "image_np = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# Fai la previsione\n",
        "prediction = model.predict(image_np)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class_name = class_names[predicted_class_index]  # Assicurati che class_names sia definito\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name}\")\n",
        "\n",
        "# Ottieni la percentuale di esattezza\n",
        "confidence = np.max(prediction) * 100\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0625d39-8eb3-484f-ca13-dc457a7ebe26",
        "id": "GnatAD5lYiF2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "Classe predetta: parmigiano_reggiano_3\n",
            "Classe predetta: parmigiano_reggiano_3 (100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (27112024 Prova modello TensorFlow)\n",
        "!pip install --upgrade numpy\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Carica il modello\n",
        "model = keras.models.load_model('image_classifier.h5')\n",
        "\n",
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carica e pre-elabora una nuova immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/pecorino_romano_0_1.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/pecorino_romano_bold_8_1.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/ctcb_it.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/granapadano_kleecks_cdn_com.png'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/Logo-pecorino-dop-100.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/Logo-pecorino-dop-200.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/parmigiano_reggiano_x_12.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "\n",
        "\n",
        "\n",
        "# Carica class_names dal file\n",
        "with open('class_names.txt', 'r') as f:\n",
        "    class_names = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = image.resize((180, 180))\n",
        "image_np = np.array(image)\n",
        "image_np = image_np / 255.0\n",
        "image_np = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# Fai la previsione\n",
        "prediction = model.predict(image_np)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class_name = class_names[predicted_class_index]  # Assicurati che class_names sia definito\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name}\")\n",
        "\n",
        "# Ottieni la percentuale di esattezza\n",
        "confidence = np.max(prediction) * 100\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98be0532-42b0-4a7f-c8eb-01cf864d5745",
        "id": "xy0MsK0GILvU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "Classe predetta: logo_dop_0\n",
            "Classe predetta: logo_dop_0 (82.98%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Carica il modello\n",
        "model = keras.models.load_model('image_classifier.h5')\n",
        "\n",
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carica e pre-elabora una nuova immagine\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = image.resize((180, 180))\n",
        "image_np = np.array(image)\n",
        "image_np = image_np / 255.0\n",
        "image_np = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# Fai la previsione\n",
        "prediction = model.predict(image_np)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class_name = class_names[predicted_class_index]  # Assicurati che class_names sia definito\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "79979844-dd36-4ad4-bcc3-2aed12a8f915",
        "id": "jYSQR3-YYm5F"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'class_names' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9749a5465b18>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpredicted_class_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mpredicted_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_class_index\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assicurati che class_names sia definito\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Classe predetta: {predicted_class_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Carica il modello\n",
        "model = keras.models.load_model('image_classifier.h5')\n",
        "\n",
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carica e pre-elabora una nuova immagine\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = image.resize((180, 180))\n",
        "image_np = np.array(image)\n",
        "image_np = image_np / 255.0\n",
        "image_np = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# Fai la previsione\n",
        "prediction = model.predict(image_np)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class_name = class_names[predicted_class_index]  # Assicurati che class_names sia definito\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2b0b66-3d60-449b-f381-4ade76566f79",
        "id": "FVZNBJ5iYpGg"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7da476914700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
            "Classe predetta: logo_dop_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Carica il modello\n",
        "model = keras.models.load_model('image_classifier.h5')\n",
        "\n",
        "# Carica e pre-elabora una nuova immagine\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso della tua immagine\n",
        "mage_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = image.resize((180, 180))\n",
        "image_np = np.array(image)\n",
        "image_np = image_np / 255.0\n",
        "image_np = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# Fai la previsione\n",
        "prediction = model.predict(image_np)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class_name = class_names[predicted_class_index]  # Assicurati che class_names sia definito\n",
        "\n",
        "print(f\"Classe predetta: {predicted_class_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j39SG4OrRKjk",
        "outputId": "81bd4698-8e63-4c04-d5d3-a7bfaa9b20c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-8f919032eec6>\", line 1, in <cell line: 1>\n",
            "    from tensorflow import keras\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 2, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 8, in <module>\n",
            "    from keras.api import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n",
            "    from keras.src.activations import deserialize\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\", line 13, in <module>\n",
            "    from keras.src import visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/__init__.py\", line 2, in <module>\n",
            "    from keras.src.visualization import plot_image_gallery\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/plot_image_gallery.py\", line 13, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 161, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 57, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 143, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-8f919032eec6>\", line 1, in <cell line: 1>\n",
            "    from tensorflow import keras\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 2, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
            "    from keras.api import visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
            "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
            "    from matplotlib import patches  # For legend patches\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 161, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 57, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 143, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8f919032eec6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso della tua immagine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/grana_padano_x_0.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zejW89N4kPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "c6e137ff-ba10-4820-e65d-ebedab0b74b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "inizio\n",
            "0\n",
            "ok_100\n",
            "Found 48 files belonging to 1 classes.\n",
            "Using 10 files for training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Value returned by __array__ is not a NumPy array",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-25d73c40492b>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# It's good practice to use a validation split when developing your model. Use 80% of the images for training and 20% for validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     64\u001b[0m   \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    332\u001b[0m             )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         dataset = paths_and_labels_to_dataset(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mimage_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mpaths_and_labels_to_dataset\u001b[0;34m(image_paths, image_size, num_channels, labels, label_mode, num_classes, interpolation, data_format, crop_to_aspect_ratio, pad_to_aspect_ratio, shuffle, shuffle_buffer_size, seed)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mpath_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         label_ds = dataset_utils.labels_to_dataset(\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mlabels_to_dataset\u001b[0;34m(labels, label_mode, num_classes)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \"\"\"\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0mlabel_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         label_ds = label_ds.map(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 134\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    733\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m ) -> ops._EagerTensorBase:\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Value returned by __array__ is not a NumPy array"
          ]
        }
      ],
      "source": [
        "# (Loghi Stefano: (dopo Image Augmentation)) Creazione modello Keras, de Keras a tfLite Quantizzato\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "from scipy import datasets\n",
        "#\n",
        "# !pip install tensorflow==2.12.0 # Installa TensorFlow 2.12.0 per compatibilità\n",
        "# !pip install tf-nightly\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install --upgrade tf-nightly\n",
        "\n",
        "# !pip install tensorflow==2.9.0 # Installa TensorFlow 2.9.0\n",
        "# !pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime # Installa il runtime di TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Download and explore the dataset\n",
        "import pathlib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# (Prova)\n",
        "data_dir = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codelab/loghi_12112024'\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(\"inizio\")\n",
        "print(image_count)\n",
        "\n",
        "print(\"ok_100\")\n",
        "\n",
        "\n",
        "# Load data using a Keras utility\n",
        "# Create a dataset\n",
        "# Define some parameters for the loader:\n",
        "\n",
        "# (Originale)\n",
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "\"\"\"\n",
        "(Probabile modifica da fare : 0.8 e 0.2)\n",
        "\n",
        "    train_ds: This line creates a training dataset from the images in data_dir.\n",
        "        validation_split=0.2: 20% of the images will be reserved for validation.\n",
        "        subset=\"training\": Specifies that this is the training subset.\n",
        "        seed=123: Sets a random seed for reproducibility.\n",
        "        image_size=(img_height, img_width): Resizes all images to the specified dimensions.\n",
        "        batch_size=batch_size: The number of images to include in each training batch.\n",
        "\n",
        "    val_ds: Similarly, this line creates a validation dataset with the same parameters as the training dataset, but using the remaining 20% of the images.\n",
        "\n",
        "\"\"\"\n",
        "# It's good practice to use a validation split when developing your model. Use 80% of the images for training and 20% for validation.\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.8,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "print(\"ok_200\")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# You can find the class names in the class_names attribute on these datasets. These correspond to the directory names in alphabetical order.\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "print(\"ok_200_10\")\n",
        "print(class_names)\n",
        "\n",
        "# Crea e apri il file txt in modalità scrittura\n",
        "with open(\"model1_txt.txt\", \"w\") as file:\n",
        "  # Scrivi ogni nome di classe su una nuova riga\n",
        "  for nome in class_names:\n",
        "    file.write(nome + \"\\n\")\n",
        "\n",
        "print(\"ok_300\")\n",
        "\n",
        "# Visualize the data\n",
        "# Here are the first nine images from the training dataset:\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "\n",
        "  # (Originale)\n",
        "  # for i in range(9):\n",
        "\n",
        "  for i in range(3):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "print(\"ok_400\")\n",
        "\n",
        "# You will pass these datasets to the Keras Model.fit method for training later in this tutorial. If you like, you can also manually iterate over the dataset and retrieve batches of images:\n",
        "\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "\n",
        "\n",
        "print(\"ok_500\")\n",
        "\n",
        "# Configure the dataset for performance\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"ok_600\")\n",
        "\n",
        "\n",
        "\n",
        "# Standardize the data\n",
        "# The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small.\n",
        "# Here, you will standardize values to be in the [0, 1] range by using tf.keras.layers.Rescaling:\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "print(\"ok_700\")\n",
        "\n",
        "# There are two ways to use this layer. You can apply it to the dataset by calling Dataset.map:\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "print(\"ok_750\")\n",
        "\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))\n",
        "print(\"ok_780\")\n",
        "\n",
        "# A basic Keras model, Create the model\n",
        "num_classes = len(class_names)\n",
        "\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(6),\n",
        "  layers.Dense(128, activation='relu'),  # Penultimo layer\n",
        "  layers.Dense(num_classes, activation='softmax')  # Ultimo layer\n",
        "])\n",
        "\n",
        "\n",
        "print(\"ok_900\")\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"ok_950\")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "print(\"ok_1000\")\n",
        "\n",
        "# Train the model\n",
        "# Train the model for 10 epochs with the Keras Model.fit method:\n",
        "\n",
        "# (Originale)\n",
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "\n",
        "print(\"ok_1500\")\n",
        "\n",
        "# Visualize training results\n",
        "# Create plots of the loss and accuracy on the training and validation sets:\n",
        "# The plots show that training accuracy and validation accuracy are off by large margins, and the model has achieved only around 60% accuracy on the validation set.\n",
        "# The following tutorial sections show how to inspect what went wrong and try to increase the overall performance of the model.\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "print(\"ok_2000\")\n",
        "\n",
        "\n",
        "\n",
        "# Data augmentation\n",
        "# (Todo Personalizzabile)\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(img_height,\n",
        "                                  img_width,\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(\"ok_2500\")\n",
        "\n",
        "# Visualize a few augmented examples by applying data augmentation to the same image several times:\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "print(\"ok_3000\")\n",
        "\n",
        "print(\"ok_3000_3000\")\n",
        "\n",
        "\n",
        "\n",
        "# Dropout\n",
        "\n",
        "# (Start) Originale\n",
        "# (Todo Personalizzabile)\n",
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, name=\"outputs\"),\n",
        "  layers.Dense(128, activation='relu'),  # Penultimo layer\n",
        "  layers.Dense(num_classes, activation='softmax')  # Ultimo layer\n",
        "\n",
        "])\n",
        "print(\"ok_3500_1\")\n",
        "model.save('model1.keras')\n",
        "# model.save('model1.h5')\n",
        "# model.save('new_saved_model.keras')            # Save to a new directory\n",
        "# model.save('my_saved_model', save_format='tf') # Save as TensorFlow SavedModel in the same directory\n",
        "# model.save('my_saved_model.keras', overwrite=True)\n",
        "print(\"ok_3500_2\")\n",
        "\n",
        "# (End) Originale\n",
        "\n",
        "print(\"ok_3500\")\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# (6+) (Prova)\n",
        "\n",
        "print(\"ok_4000\")\n",
        "model.summary()\n",
        "\n",
        "print(\"ok_4500\")\n",
        "\n",
        "epochs = 15\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "\n",
        "print(\"ok_5000\")\n",
        "\n",
        "\n",
        "# Visualize training results\n",
        "# After applying data augmentation and tf.keras.layers.Dropout, there is less overfitting than before, and training and validation accuracy are closer aligned:\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"ok_6000\")\n",
        "# Salva il modello in formato HDF5\n",
        "# model.save('loghi_11112024_h5.h5')\n",
        "\n",
        "\"\"\"\n",
        "(Start) Converte il modello da Keras a tfLite quantizzato\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "model = tf.keras.models.load_model('my_saved_model_3.keras') # Sostituisci con il\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Salva il modello TensorFlow Lite non quantizzato\n",
        "with open('model_non_quantizzato_3.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Rappresentante Dataset\n",
        "def representative_dataset_gen():\n",
        "    for _ in range(100):  # Genera 100 immagini di esempio\n",
        "        yield [np.random.uniform(0.0, 1.0, size=(1, img_height, img_width, 3)).astype(np.float32)]  # Sostituisci con il tuo generatore di dati rappresentativo\n",
        "\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # Imposta il tipo di input per l'inferenza\n",
        "converter.inference_output_type = tf.int8  # Imposta il tipo di output per l'inferenza\n",
        "\n",
        "tflite_model_quantizzato = converter.convert()\n",
        "\n",
        "# Salva il modello TensorFlow Lite quantizzato\n",
        "with open('model_quantizzato_3.tflite', 'wb') as f:\n",
        "  f.write(tflite_model_quantizzato)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "(End) Converte il modello da Keras a tfLite quantizzato\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carica il modello h5\n",
        "model = tf.keras.models.load_model('italian2_products.h5')\n",
        "\n",
        "# Converti il modello in tflite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Salva il modello tflite\n",
        "with open('nome_del_mio_modello.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Specifica il percorso completo del file\n",
        "filepath = '/Users/massimosenatore/app_development/wksPython/italianSolvingTfUnit/mio_modello.h5'\n",
        "\n",
        "# model.save(filepath)\n",
        "\n",
        "print(\"ok_6600\")\n",
        "\n",
        "model1 = Sequential(filepath)  # Sostituisci 'mio_modello.h5' con il nome del tuo file modello\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "converter = tf.TFLiteConverter.from_saved_model(filepath)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"/drive/My Drive/FSD_modelV09A.tflite\", \"wb\").write(tflite_model)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "print(\"ok_6800\")\n",
        "converter = tf.TFLiteConverter.from_saved_model('mio_modello.h5')\n",
        "print(\"ok_6900\")\n",
        "tflite_model = converter.convert()\n",
        "with open('mio_modello_quantizzato.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "print(\"ok_7000\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" (DaCodeLabGemini)\n",
        "# Conversione in tfLite Quantizzato\n",
        "# Prendi i primi 100 esempi dal dataset di validazione\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "val_ds = val_ds.take(100)\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in val_ds.batch(1).take(100):\n",
        "    print(\"Input shape:\", input_value.shape)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.experimental_new_converter = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model_quantized = converter.convert()  # Esegue la conversione\n",
        "\n",
        "# Salva il modello quantizzato in un file\n",
        "with open('modello_quantizzato.tflite', 'wb') as f:\n",
        "  f.write(tflite_model_quantized)\n",
        " (DaCodeLabGemini)\n",
        "\"\"\"\n"
      ]
    },
    {
      "source": [
        "# Carica l'immagine di esempio\n",
        "img_path = '/percorso/della/tua/immagine.jpg'\n",
        "img = tf.keras.utils.load_img(\n",
        "    img_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Crea un batch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "80WjXD--rcQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Carica l'immagine di esempio\n",
        "img_path = '/percorso/della/tua/immagine.jpg'\n",
        "img = tf.keras.utils.load_img(\n",
        "    img_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Crea un batch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mPMtOv9Crr__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Eseguire Seconda)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carica il modello h5\n",
        "model = tf.keras.models.load_model('italian2_products.h5')\n",
        "\n",
        "# Converti il modello in tflite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Salva il modello tflite\n",
        "with open('italian2_products.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "8xGeRZoFAGhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Eseguire Terza) (Loghi Stefano(Creazione h5) : (dopo Creazione modello Keras, da Keras a tfLite Quantizzato)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carica il modello h5 (se necessario)\n",
        "model = tf.keras.models.load_model('my_saved_model_4.h5')\n",
        "\n",
        "print(\"_x10_100\")\n",
        "\n",
        "# Crea il convertitore TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "print(\"_x10_200\")\n",
        "\n",
        "\n",
        "# Abilita la quantizzazione\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "print(\"_x10_300\")\n",
        "\n",
        "\n",
        "# (Opzionale) Specifica i tipi di input/output per la quantizzazione\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "print(\"_x10_400\")\n",
        "\n",
        "# (Opzionale) Fornisci dati rappresentativi per la calibrazione della quantizzazione\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in val_ds.take(100):  # Rimuovi .batch(1)\n",
        "    print(\"Generazione dati: \", input_value.shape)\n",
        "    yield [input_value]\n",
        "\n",
        "print(\"_x10_500\")\n",
        "\n",
        "gen = representative_data_gen()\n",
        "\n",
        "print(\"_x10_600\")\n",
        "\n",
        "for data in gen:\n",
        "  print(data[0].shape)\n",
        "\n",
        "print(\"_x10_700\")\n",
        "converter.representative_dataset = representative_data_gen\n",
        "print(\"_x20_800\")\n",
        "\n",
        "# Converti il modello in formato tflite quantizzato\n",
        "tflite_model_quantized = converter.convert()\n",
        "print(\"_x30_900\")\n",
        "\n",
        "# Salva il modello quantizzato\n",
        "with open('my_saved_model_4_qnt.tflite', 'wb') as f:\n",
        "  f.write(tflite_model_quantized)\n",
        "print(\"_x40_1000\")\n",
        "\n"
      ],
      "metadata": {
        "id": "boQ1eH8YA-Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50850cc-fea4-4ca6-9267-12b91126df48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_x10_100\n",
            "_x10_200\n",
            "_x10_300\n",
            "_x10_400\n",
            "_x10_500\n",
            "_x10_600\n",
            "Generazione dati:  (21, 180, 180, 3)\n",
            "(21, 180, 180, 3)\n",
            "_x10_700\n",
            "_x20_800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generazione dati:  (21, 180, 180, 3)\n",
            "_x30_900\n",
            "_x40_1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow  # Installa la libreria Pillow per la manipolazione delle immagini\n",
        "\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso della cartella contenente le immagini JPG\n",
        "cartella_immagini = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/loghi_12112024/daRidimensionare/'  # Sostituisci con il percorso effettivo\n",
        "\n",
        "# Dimensione desiderata\n",
        "new_size = (180, 180)\n",
        "\n",
        "# Loop su tutte le immagini nella cartella\n",
        "for filename in os.listdir(cartella_immagini):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # Percorso completo dell'immagine\n",
        "        image_path = os.path.join(cartella_immagini, filename)\n",
        "\n",
        "        # Apri l'immagine con Pillow\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Ridimensiona l'immagine\n",
        "        resized_image = image.resize(new_size)\n",
        "\n",
        "        # Salva l'immagine ridimensionata (sovrascrivendo quella originale)\n",
        "        resized_image.save(image_path)\n",
        "\n",
        "print('Ridimensionamento completato!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6ZwJZWuLcC",
        "outputId": "a6306f0b-c313-41fe-aaa6-0e5b83bf54d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Ridimensionamento completato!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Eseguire Quarta)\n",
        "# Questo codice va eseguito, dopo aver creato il modello convertito in tfLite quantizzato\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specifica il percorso dell'immagine su Google Drive\n",
        "# (wafer)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/wafer/20240626_151222.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (spesottiNettare)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/spesottinettare/20240626_152153.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/spaghetti/20240618_095240.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (spesottinettare)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121339.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (mulleryogurt)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121804.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (spaghetti)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121150.jpg'  # Sostituisci con il percorso corretto\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240620_135405.jpg'  # Sostituisci con il percorso corretto\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (girella)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121025.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (wafer)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_120513.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (pomodori)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121716.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "\n",
        "# (Auricchio)(Foto non nel modello)\n",
        "# (Test non ok o comunque da analizzare e seguire i consigli di ottimizzazione)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240627_094621.jpg'  # Sostituisci con il percorso corretto\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240627_093600.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (Focaccina)\n",
        "# (Test ok)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240627_090022.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# (belgioso)\n",
        "# (Test ok con modello che ha la classe fake)\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_152623.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# Carica l'immagine e ridimensionala alle dimensioni attese dal modello\n",
        "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
        "\n",
        "# Converti l'immagine in un array NumPy\n",
        "input_data = tf.keras.preprocessing.image.img_to_array(image)\n",
        "\n",
        "# Quantizza l'immagine (se hai quantizzato i dati di addestramento)\n",
        "input_data = input_data.astype(np.uint8)  # Adatta il tipo di dati se necessario\n",
        "\n",
        "# Aggiungi una dimensione batch\n",
        "input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "# Ottieni gli indici degli input e degli output del modello\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Imposta i dati di input (l'immagine caricata)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\n",
        "# Ottieni i risultati dell'inferenza\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "predicted_class = np.argmax(output_data)\n",
        "print(\"Classe predetta:\", class_names[predicted_class])\n",
        "\n",
        "# Ottieni la probabilità della classe predetta\n",
        "predicted_probability = np.max(output_data)\n",
        "\n",
        "# Converti la probabilità in percentuale\n",
        "# Calcola la somma di tutte le probabilità predette\n",
        "# Calcola la percentuale di predizione\n",
        "# Calcola la somma di tutte le probabilità predette\n",
        "total_probability = np.sum(output_data)\n",
        "\n",
        "# Calcola la percentuale di predizione\n",
        "predicted_percentage = (predicted_probability / total_probability) * 100\n",
        "print(\"Percentuale di predizione:\", predicted_percentage, \"%\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Calcola l'entropia\n",
        "entropy = -np.sum(output_data * np.log2(output_data + 1e-10))  # Aggiungi un piccolo valore per evitare log(0)\n",
        "\n",
        "print(\"Entropia:\", entropy)\n",
        "\n",
        "# Ottieni le due probabilità più alte\n",
        "top_2_probabilities = np.sort(output_data)[-2:]\n",
        "\n",
        "# Calcola il margine di confidenza\n",
        "if len(top_2_probabilities) > 1:\n",
        "  confidence_margin = top_2_probabilities[1] - top_2_probabilities[0]\n",
        "  print(\"Margine di confidenza:\", confidence_margin)\n",
        "else:\n",
        "  print(\"Non è possibile calcolare il margine di confidenza: solo una classe predetta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X6PmPpgKQc2",
        "outputId": "8d77c966-ba02-4ddc-8e44-9d6251ff169c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Classe predetta: pecorino_romano_8\n",
            "Percentuale di predizione: 26.953125 %\n",
            "Entropia: -1536.3771651867382\n",
            "Non è possibile calcolare il margine di confidenza: solo una classe predetta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Eseguire dopo la Quarta per ottenere le percentuali su tutte le classi)\n",
        "import numpy as np\n",
        "\n",
        "# ... (resto del tuo codice)\n",
        "\n",
        "# Esegui la predizione\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "# Calcola le probabilità per ogni classe\n",
        "probabilities = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
        "probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
        "\n",
        "# Stampa le probabilità\n",
        "for i, probability in enumerate(probabilities[0]):\n",
        "    print(f\"Classe {class_names[i]}: {probability * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0z9FmtuBCWI",
        "outputId": "fcaf9cbe-6b2e-4428-d8df-6f8fd80d783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "Classe fakebelgioioso: 15.02%\n",
            "Classe girella: 11.98%\n",
            "Classe mulleryogurt: 12.05%\n",
            "Classe pomodori: 11.91%\n",
            "Classe spaghetti: 25.21%\n",
            "Classe spesottinettare: 11.91%\n",
            "Classe wafer: 11.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tflite-runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c10SBF10n81Z",
        "outputId": "2cccc45f-09a5-4cb2-ba05-8aff96497c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tflite-runtime\n",
            "  Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite-runtime) (1.23.5)\n",
            "Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow  # Installa la libreria Pillow per la manipolazione delle immagini\n",
        "\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorso della cartella contenente le immagini JPG\n",
        "cartella_immagini = '/content/drive/MyDrive/tua_cartella'  # Sostituisci con il percorso effettivo\n",
        "\n",
        "# Dimensione desiderata\n",
        "new_size = (180, 180)\n",
        "\n",
        "# Loop su tutte le immagini nella cartella\n",
        "for filename in os.listdir(cartella_immagini):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # Percorso completo dell'immagine\n",
        "        image_path = os.path.join(cartella_immagini, filename)\n",
        "\n",
        "        # Apri l'immagine con Pillow\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Ridimensiona l'immagine\n",
        "        resized_image = image.resize(new_size)\n",
        "\n",
        "        # Salva l'immagine ridimensionata (sovrascrivendo quella originale)\n",
        "        resized_image.save(image_path)\n",
        "\n",
        "print('Ridimensionamento completato!')"
      ],
      "metadata": {
        "id": "TLg1mn5CswSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Da LoghiStefano) (Da riperfezionare con i parametri prova)\n",
        "# (Eseguire Quinta)\n",
        "# (Oppure (Quarta))\n",
        "# (E leggere il file di testo ed il modello in usa sessione apparte)\n",
        "\n",
        "# Predizione dal modello tensorflow lite quantizzato, precedentemente salvato\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Carica il modello TFLite quantizzato\n",
        "# interpreter = tf.lite.Interpreter(model_path='my_saved_model_4_qtn.tflite')\n",
        "# interpreter.allocate_tensors()\n",
        "\n",
        "# Percorso del modello\n",
        "# model_path = os.path.join(os.path.abspath(''), 'sample_data', 'modello.tflite')\n",
        "\n",
        "# Crea un'istanza dell'interprete TensorFlow Lite\n",
        "# interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Definisci interpreter\n",
        "# model_path = os.path.join(os.path.abspath(''), 'sample_data', 'modello.tflite')\n",
        "model_path= '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/modello.tflite'  # Sostituisci con il percorso corretto\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path) # Assegnazione corretta\n",
        "\n",
        "\n",
        "# Alloca i tensori\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ottieni i dettagli di input e output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specifica il percorso dell'immagine su Google Drive\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/wafer/20240626_151222.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/spesottinettare/20240626_152153.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/spaghetti/20240618_095240.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121804.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121150.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_121025.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "# image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/20240705_120513.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "image_path = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/logo_dop_0_0.jpg'  # Sostituisci con il percorso corretto\n",
        "\n",
        "img_width = 180\n",
        "img_height = 180\n",
        "\n",
        "# Carica l'immagine e ridimensionala alle dimensioni attese dal modello\n",
        "image = Image.open(image_path).resize((img_width, img_height))  # Sostituisci img_width e img_height con le dimensioni corrette\n",
        "\n",
        "# Converti l'immagine in un array NumPy\n",
        "input_data = np.array(image)\n",
        "\n",
        "# Aggiungi una dimensione batch (se necessario)\n",
        "input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "# Quantizza l'immagine (se hai quantizzato i dati di addestramento)\n",
        "input_data = input_data.astype(np.uint8)\n",
        "\n",
        "\n",
        "# Imposta i dati di input (l'immagine caricata)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\n",
        "# Ottieni i risultati dell'inferenza\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Ottieni l'indice della classe predetta\n",
        "predicted_class = np.argmax(output_data)\n",
        "\n",
        "# Ottieni il nome della classe predetta (se hai una lista di nomi di classi)\n",
        "# class_names = [...]  # Sostituisci con la tua lista di nomi di classi\n",
        "# Apri il file in modalità lettura\n",
        "with open('/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/fotoDaProvare/loghi_12112024_txt_4.txt', 'r') as f:\n",
        "  # Leggi tutte le righe del file e rimuovi eventuali spazi vuoti\n",
        "  class_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Stampa la lista dei nomi delle classi\n",
        "print(class_names)\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(\"Classe predetta:\", predicted_class_name)\n",
        "\n",
        "# Probabilità di predizione per ogni classe dal modello quantizzato.\n",
        "\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\n",
        "# Ottieni i risultati dell'inferenza\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Calcola le probabilità per ogni classe usando softmax di NumPy\n",
        "probabilities = np.exp(output_data - np.max(output_data, axis=1, keepdims=True))\n",
        "probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
        "\n",
        "# Stampa le probabilità\n",
        "for i, probability in enumerate(probabilities[0]):\n",
        "    print(f\"Classe {class_names[i]}: {probability * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kjyrLehKhny",
        "outputId": "5e6db397-3ec9-47bc-ac2f-9dcfdd185491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['grana_padano_3', 'logo_dop_0', 'pecorino_romano_0', 'pecorino_romano_8']\n",
            "Classe predetta: pecorino_romano_8\n",
            "Classe grana_padano_3: nan%\n",
            "Classe logo_dop_0: nan%\n",
            "Classe pecorino_romano_0: nan%\n",
            "Classe pecorino_romano_8: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8dfde8d513d0>:119: RuntimeWarning: overflow encountered in exp\n",
            "  probabilities = np.exp(output_data - np.max(output_data, axis=1, keepdims=True))\n",
            "<ipython-input-4-8dfde8d513d0>:120: RuntimeWarning: invalid value encountered in divide\n",
            "  probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Carica il modello TFLite dal file\n",
        "interpreter = tf.lite.Interpreter(model_path='italian2_products.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "oE-0w6mdBDsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "691e6e9f-94ea-4827-cc3b-d621637307dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not open 'italian2_products.tflite'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7b63cd2fa91d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Carica il modello TFLite dal file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'italian2_products.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering, experimental_default_delegate_latest_features)\u001b[0m\n\u001b[1;32m    471\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_custom_op_registerers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m       ]\n\u001b[0;32m--> 473\u001b[0;31m       self._interpreter = _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[0m\u001b[1;32m    474\u001b[0m           \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m           \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open 'italian2_products.tflite'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni gli indici degli input e degli output del modello\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# (Prova)\n",
        "data_dirFileImage = '/content/drive/MyDrive/PROGRAMMAZIONE/AppItalianSolving/codeLab/italian2_products/wafer/20240626_151222.jpg'\n",
        "\n",
        "image = tf.keras.preprocessing.image.load_img(data_dirFileImage, target_size=(img_height, img_width))\n",
        "input_data = tf.keras.preprocessing.image.img_to_array(image)\n",
        "\n",
        "# Converti l'array NumPy in tipo uint8\n",
        "input_data = input_data.astype(np.uint8)\n",
        "\n",
        "# Aggiungi una dimensione batch (se necessario)\n",
        "input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "# Ora puoi utilizzare input_data nella chiamata a set_tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Imposta i dati di input\n",
        "interpreter.set_tensor(input_details[0]['index'], data_dirFileImage)\n",
        "\n",
        "# Esegui l'inferenza\n",
        "interpreter.invoke()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "N1Xeh0vfBJw-",
        "outputId": "657a5eb9-2f22-42e4-a774-57849da4cca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n# Imposta i dati di input\\ninterpreter.set_tensor(input_details[0]['index'], data_dirFileImage)\\n\\n# Esegui l'inferenza\\ninterpreter.invoke()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carica il modello h5 (se necessario)\n",
        "model = tf.keras.models.load_model('italian2_products.h5')\n",
        "\n",
        "# Crea il convertitore TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Abilita la quantizzazione\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Definisci la funzione per generare dati rappresentativi\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in val_ds.batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "# Imposta i dati rappresentativi per la quantizzazione\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Specifica i tipi di input/output per la quantizzazione\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "\"\"\"\n",
        "# Esegui la conversione\n",
        "tflite_model_quantized = converter.convert()\n",
        "\n",
        "# Salva il modello TFLite quantizzato\n",
        "with open('nome_del_mio_modello_quantizzato.tflite', 'wb') as f:\n",
        "  f.write(tflite_model_quantized)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "JH2R_EAp8s-q",
        "outputId": "a3d4229e-3064-46e5-e088-5dcea491e18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Esegui la conversione\\ntflite_model_quantized = converter.convert()\\n\\n# Salva il modello TFLite quantizzato\\nwith open('nome_del_mio_modello_quantizzato.tflite', 'wb') as f:\\n  f.write(tflite_model_quantized)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tflite_model_quantized = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Jt4fTe5r9XOO",
        "outputId": "71f622a7-a964-4927-ab6b-ec7e1b6c877d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "tensorflow/lite/kernels/conv.cc:346 input->dims->size != 4 (5 != 4)Node number 0 (CONV_2D) failed to prepare.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6afee6e60d38>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtflite_model_quantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \"\"\"\n\u001b[0;32m-> 1601\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1580\u001b[0m       )\n\u001b[1;32m   1581\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         return super(TFLiteKerasModelConverterV2, self).convert(\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     )\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m     return self._optimize_tflite_model(\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_io\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mq_allow_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allow_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mq_variable_quantization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_mlir_variable_quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         model = self._quantize(\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mq_in_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_calibrate_only\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m       calibrated = calibrate_quantize.calibrate(\n\u001b[0m\u001b[1;32m    736\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mdataset_gen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mgenerates\u001b[0m \u001b[0mcalibration\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \"\"\"\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36m_feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    141\u001b[0m             )\n\u001b[1;32m    142\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0msignature_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/conv.cc:346 input->dims->size != 4 (5 != 4)Node number 0 (CONV_2D) failed to prepare."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml-dtypes~=0.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzkuMTonNzPr",
        "outputId": "537c1d7a-6973-49e1-d0c1-5944a4ed921f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ml-dtypes~=0.2.0) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import tensorflow.lite as tflite\n",
        "print(tflite.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "r40HfEf8MM9Q",
        "outputId": "2f0daa20-28da-4a13-ae9f-4f6a062c9563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0-dev20240626\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow._api.v2.lite' has no attribute '__version__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cccb8aad4b7a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.lite' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tf-nightly"
      ],
      "metadata": {
        "id": "MlpqQOREM5G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWDCledVQvpf",
        "outputId": "cf1a4580-bcc5-42d8-db15-7d15dee868f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.15.4)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.2.2)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv myenv"
      ],
      "metadata": {
        "id": "7UgvEpJSQ72P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source myenv/bin/activate"
      ],
      "metadata": {
        "id": "aUqeZVr1RFyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tf-nightly"
      ],
      "metadata": {
        "id": "B_Em0hbBRLeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.lite as tflite\n",
        "print(tflite.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "5GANipkSRi7J",
        "outputId": "ae43615b-ae62-4d50-acf1-1e230fb3b12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow._api.v2.lite' has no attribute '__version__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-383c46c810cc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.lite' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall tf-nightly"
      ],
      "metadata": {
        "id": "4GZU0n37R-GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi>=0.16"
      ],
      "metadata": {
        "id": "rrl-fvTCSvKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-nvrtc-cu12==12.1.45 nvidia-cuda-runtime-cu12==12.1.65 nvidia-cudnn-cu12==8.9.1.23"
      ],
      "metadata": {
        "id": "QgmzIantS_MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "id": "8cG9GDhGT2P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_g1xjDFLT2N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "id": "7HXd8lInULZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv myenv"
      ],
      "metadata": {
        "id": "_mXjWxAjUOpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source myenv/bin/activate"
      ],
      "metadata": {
        "id": "ED-lazrtUSCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGRVFbQPUcSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tf-nightly"
      ],
      "metadata": {
        "id": "rxVbVkzpUbZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.lite as tflite\n",
        "print(tflite.__version__)\n"
      ],
      "metadata": {
        "id": "qmKA7BSBUofz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall tf-nightly"
      ],
      "metadata": {
        "id": "yRiEDbQ6U1xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source myenv/bin/activate\n"
      ],
      "metadata": {
        "id": "d03-G_W4XD-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source myenv/bin/activate\n"
      ],
      "metadata": {
        "id": "hxUh7HkTXah1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source myenv/bin/activate\n"
      ],
      "metadata": {
        "id": "iE4tr9TnXuAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze\n",
        "\n"
      ],
      "metadata": {
        "id": "AbRNx5usZ6Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "id": "HncChejWcBND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "PrhK77lfchQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-support==0.4.3"
      ],
      "metadata": {
        "id": "PN9vU1mBcTzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "id": "dJPA7GOVcwTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8u7vyLbc5rb",
        "outputId": "ea75bc3c-61c4-449b-8863-986aa5785ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.0\n",
            "Uninstalling numpy-1.26.0:\n",
            "  Successfully uninstalled numpy-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "id": "Ysaq8LEqc_oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XRqV0IRdSsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "zrylRh7_dSEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKEQcCpTd3l-",
        "outputId": "c52d37f7-c048-4251-9c6f-b65c22e922a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Successfully uninstalled tensorflow-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "cYn44IdTeAae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tf-keras"
      ],
      "metadata": {
        "id": "xTv4l5S2egIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hdhOOAheqAm",
        "outputId": "cd601088-5cab-400d-f612-0984fcbfb5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.16.2\n",
            "Uninstalling tensorflow-2.16.2:\n",
            "  Successfully uninstalled tensorflow-2.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICChTaWae9b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tf-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmzHiLE8fH1z",
        "outputId": "15ea7ea5-8e30-40d6-8813-975622de1b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tf_keras\n",
            "Version: 2.16.0\n",
            "Summary: Deep learning for humans.\n",
            "Home-page: https://keras.io/\n",
            "Author: Keras team\n",
            "Author-email: keras-users@googlegroups.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: tensorflow\n",
            "Required-by: tensorflow-hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA08wMN8fXss",
        "outputId": "0ddc82b6-9912-4b6a-9a84-4bc7d06b968b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.16 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.16\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.16.1"
      ],
      "metadata": {
        "id": "_7mJ9jHif4fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "OnADuB6DgWnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IpwpMY_gYWH",
        "outputId": "a7e15e21-582b-4eee-b98a-c2e3c922c045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-support==0.4.4"
      ],
      "metadata": {
        "id": "J--xf_aAgkEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "print(tflite.__version__)"
      ],
      "metadata": {
        "id": "E8yf_dAng3QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-runtime"
      ],
      "metadata": {
        "id": "ApdIXciCi6h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUodzPtKjaXb",
        "outputId": "d832bf40-72cb-4d05-f02e-e452511932ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-runtime\n"
      ],
      "metadata": {
        "id": "vKnNmXP-jf0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "print(tflite.__version__)"
      ],
      "metadata": {
        "id": "C5MFFEPHjsEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}